{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.2.1.\u001b[31m9000\u001b[39m     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.3     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 2.1.3          \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 0.8.3     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.0.0          \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1          \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.4.0     \n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Loaded gbm 2.1.5\n",
      "\n",
      "randomForest 4.6-14\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘mltools’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:tidyr’:\n",
      "\n",
      "    replace_na\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘data.table’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    between, first, last\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    transpose\n",
      "\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: ‘caret’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:SDMTools’:\n",
      "\n",
      "    sensitivity, specificity\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    lift\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:httr’:\n",
      "\n",
      "    progress\n",
      "\n",
      "\n",
      "Installing package into ‘/usr/local/lib/R/site-library’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'agriculture-loan-prediction'"
      ],
      "text/latex": [
       "'agriculture-loan-prediction'"
      ],
      "text/markdown": [
       "'agriculture-loan-prediction'"
      ],
      "text/plain": [
       "[1] \"agriculture-loan-prediction\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Importing packages\n",
    "\n",
    "# This R environment comes with all of CRAN and many other helpful packages preinstalled.\n",
    "# You can see which packages are installed by checking out the kaggle/rstats docker image: \n",
    "# https://github.com/kaggle/docker-rstats\n",
    "\n",
    "library(tidyverse) # metapackage with lots of helpful functions\n",
    "library(SDMTools)\n",
    "library(gbm)\n",
    "library(randomForest)\n",
    "library(zipcode)\n",
    "library(mltools)\n",
    "library(data.table)\n",
    "library(caret)\n",
    "\n",
    "#devtools::install_github(\"brews/bayfoxr\", lib = \"/kaggle/working\")\n",
    "install.packages(\"tidycensus\")\n",
    "library(tidycensus)\n",
    "\n",
    "## Running code\n",
    "\n",
    "# In a notebook, you can run a single code cell by clicking in the cell and then hitting \n",
    "# the blue arrow to the left, or by clicking in the cell and pressing Shift+Enter. In a script, \n",
    "# you can run code by highlighting the code you want to run and then clicking the blue arrow\n",
    "# at the bottom of this window.\n",
    "\n",
    "## Reading in files\n",
    "\n",
    "# You can access files from datasets you've added to this kernel in the \"../input/\" directory.\n",
    "# You can see the files added to this kernel by running the code below. \n",
    "\n",
    "list.files(path = \"../input\")\n",
    "\n",
    "## Saving data\n",
    "\n",
    "# If you save any files or images, these will be put in the \"output\" directory. You \n",
    "# can see the output directory by committing and running your kernel (using the \n",
    "# Commit & Run button) and then checking out the compiled version of your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in files\n",
    "train=read.csv(\"../input/agriculture-loan-prediction/train_comp1_2020.csv\",stringsAsFactors=FALSE)\n",
    "test=read.csv(\"../input/agriculture-loan-prediction/test_comp1_2020.csv\",stringsAsFactors=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n"
     ]
    }
   ],
   "source": [
    "getmode <- function(v) {\n",
    "   uniqv <- unique(v)\n",
    "   uniqv[which.max(tabulate(match(v, uniqv)))]\n",
    "}\n",
    "\n",
    "#Remove the nulls\n",
    "#head(train$TYCornUnits[!is.numeric(train$TYCornUnits)])\n",
    "\n",
    "train$TYCornUnits=abs(as.numeric(train$TYCornUnits))\n",
    "train$TYCornUnits[is.na(train$TYCornUnits)]=median(train$TYCornUnits[!is.na(train$TYCornUnits)])\n",
    "train$TYBeanUnits=as.numeric(train$TYBeanUnits)\n",
    "train$TYBeanUnits[is.na(train$TYBeanUnits)]=median(train$TYBeanUnits[!is.na(train$TYBeanUnits)])\n",
    "train$TYWheatUnits=as.numeric(train$TYWheatUnits)\n",
    "train$TYWheatUnits[is.na(train$TYWheatUnits)]=median(train$TYWheatUnits[!is.na(train$TYWheatUnits)])\n",
    "train$LYCornUnits=as.numeric(train$LYCornUnits)\n",
    "train$LYCornUnits[is.na(train$LYCornUnits)]=median(train$LYCornUnits[!is.na(train$LYCornUnits)])\n",
    "train$LYBeanUnits=as.numeric(train$LYBeanUnits)\n",
    "train$LYBeanUnits[is.na(train$LYBeanUnits)]=median(train$LYBeanUnits[!is.na(train$LYBeanUnits)])\n",
    "train$LYWheatUnits=as.numeric(train$LYWheatUnits)\n",
    "train$LYWheatUnits[is.na(train$LYWheatUnits)]=median(train$LYWheatUnits[!is.na(train$LYWheatUnits)])\n",
    "train$AT28=as.numeric(train$AT28)\n",
    "train$AT28[is.na(train$AT28)]=median(train$AT28[!is.na(train$AT28)])\n",
    "train$AT33=as.numeric(train$AT33)\n",
    "train$AT33[is.na(train$AT33)]=median(train$AT33[!is.na(train$AT33)])\n",
    "train$AT36=as.numeric(train$AT36)\n",
    "train$AT36[is.na(train$AT36)]=median(train$AT36[!is.na(train$AT36)])\n",
    "train$BC33=as.numeric(train$BC33)\n",
    "train$BC33[is.na(train$BC33)]=median(train$AT36[!is.na(train$BC33)])\n",
    "train$BC98=as.numeric(train$BC98)\n",
    "train$BC98[is.na(train$BC98)]=median(train$BC98[!is.na(train$BC98)])\n",
    "train$G068=as.numeric(train$G068)\n",
    "train$G068[is.na(train$G068)]=median(train$G068[!is.na(train$G068)])\n",
    "train$G091=as.numeric(train$G091)\n",
    "train$G091[is.na(train$G091)]=median(train$G091[!is.na(train$G091)])\n",
    "train$G093=as.numeric(train$G093)\n",
    "train$G093[is.na(train$G093)]=median(train$G093[!is.na(train$G093)])\n",
    "train$G094=as.numeric(train$G094)\n",
    "train$G094[is.na(train$G094)]=median(train$G094[!is.na(train$G094)])\n",
    "train$G096=as.numeric(train$G096)\n",
    "train$G096[is.na(train$G096)]=median(train$G096[!is.na(train$G096)])\n",
    "train$MT28=as.numeric(train$MT28)\n",
    "train$MT28[is.na(train$MT28)]=median(train$MT28[!is.na(train$MT28)])\n",
    "train$MT36=as.numeric(train$MT36)\n",
    "train$MT36[is.na(train$MT36)]=median(train$MT36[!is.na(train$MT36)])\n",
    "train$RE34=as.numeric(train$RE34)\n",
    "train$RE34[is.na(train$RE34)]=median(train$RE34[!is.na(train$RE34)])\n",
    "train$S063=as.numeric(train$S063)\n",
    "train$S063[is.na(train$S063)]=median(train$S063[!is.na(train$S063)])\n",
    "train$Zip_Code=as.numeric(train$Zip_Code)\n",
    "train$Zip_Code[is.na(train$Zip_Code)]=getmode(train$Zip_Code[!is.na(train$Zip_Code)])\n",
    "\n",
    "#train$TYCornUnits=log(abs(train$TYCornUnits+1))\n",
    "#train$TYBeanUnits=log(abs(train$TYBeanUnits+1))\n",
    "#train$TYWheatUnits=log(abs(train$TYWheatUnits+1))\n",
    "#train$LYCornUnits=log(abs(train$LYCornUnits+1))\n",
    "#train$LYBeanUnits=log(abs(train$LYBeanUnits+1))\n",
    "#train$LYWheatUnits=log(abs(train$LYWheatUnits+1))\n",
    "#train$AT28=log(abs(train$AT28+1))\n",
    "#train$AT33=log(abs(train$AT33+1))\n",
    "#train$AT36=log(abs(train$AT36+1))\n",
    "#train$BC33=log(abs(train$BC33+1))\n",
    "#train$BC98=log(abs(train$BC98+1))\n",
    "#train$MT28=log(abs(train$MT28+1))\n",
    "#train$MT36=log(abs(train$MT36+1))\n",
    "#train$RE34=log(abs(train$RE34+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n"
     ]
    }
   ],
   "source": [
    "test$TYCornUnits=abs(as.numeric(test$TYCornUnits))\n",
    "test$TYCornUnits[is.na(test$TYCornUnits)]=median(train$TYCornUnits)\n",
    "test$TYBeanUnits=as.numeric(test$TYBeanUnits)\n",
    "test$TYBeanUnits[is.na(test$TYBeanUnits)]=median(train$TYBeanUnits)\n",
    "test$TYWheatUnits=as.numeric(test$TYWheatUnits)\n",
    "test$TYWheatUnits[is.na(test$TYWheatUnits)]=median(train$TYWheatUnits)\n",
    "test$LYCornUnits=as.numeric(test$LYCornUnits)\n",
    "test$LYCornUnits[is.na(test$LYCornUnits)]=median(train$LYCornUnits)\n",
    "test$LYBeanUnits=as.numeric(test$LYBeanUnits)\n",
    "test$LYBeanUnits[is.na(test$LYBeanUnits)]=median(train$LYBeanUnits)\n",
    "test$LYWheatUnits=as.numeric(test$LYWheatUnits)\n",
    "test$LYWheatUnits[is.na(test$LYWheatUnits)]=median(train$LYWheatUnits)\n",
    "test$AT28=as.numeric(test$AT28)\n",
    "test$AT28[is.na(test$AT28)]=median(train$AT28)\n",
    "test$AT33=as.numeric(test$AT33)\n",
    "test$AT33[is.na(test$AT33)]=median(train$AT33)\n",
    "test$AT36=as.numeric(test$AT36)\n",
    "test$AT36[is.na(test$AT36)]=median(train$AT36)\n",
    "test$BC33=as.numeric(test$BC33)\n",
    "test$BC33[is.na(test$BC33)]=median(train$BC33)\n",
    "test$BC98=as.numeric(test$BC98)\n",
    "test$BC98[is.na(test$BC98)]=median(train$BC98)\n",
    "test$G068=as.numeric(test$G068)\n",
    "test$G068[is.na(test$G068)]=median(train$G068)\n",
    "test$G091=as.numeric(test$G091)\n",
    "test$G091[is.na(test$G091)]=median(train$G091)\n",
    "test$G093=as.numeric(test$G093)\n",
    "test$G093[is.na(test$G093)]=median(train$G093)\n",
    "test$G094=as.numeric(test$G094)\n",
    "test$G094[is.na(test$G094)]=median(train$G094)\n",
    "test$G096=as.numeric(test$G096)\n",
    "test$G096[is.na(test$G096)]=median(train$G096)\n",
    "test$MT28=as.numeric(test$MT28)\n",
    "test$MT28[is.na(test$MT28)]=median(train$MT28)\n",
    "test$MT36=as.numeric(test$MT36)\n",
    "test$MT36[is.na(test$MT36)]=median(train$MT36)\n",
    "test$RE34=as.numeric(test$RE34)\n",
    "test$RE34[is.na(test$RE34)]=median(train$RE34)\n",
    "test$S063=as.numeric(test$S063)\n",
    "test$S063[is.na(test$S063)]=median(train$S063)\n",
    "\n",
    "\n",
    "#test$TYCornUnits=log(abs(test$TYCornUnits+.01))\n",
    "#test$TYBeanUnits=log(abs(test$TYBeanUnits+.01))\n",
    "#test$TYWheatUnits=log(abs(test$TYWheatUnits+.01))\n",
    "#test$LYCornUnits=log(abs(test$LYCornUnits+.01))\n",
    "#test$LYBeanUnits=log(abs(test$LYBeanUnits+.01))\n",
    "#test$LYWheatUnits=log(abs(test$LYWheatUnits+.01))\n",
    "#test$AT28=log(abs(test$AT28+.01))\n",
    "#test$AT33=log(abs(test$AT33+.01))\n",
    "#test$AT36=log(abs(test$AT36+.01))\n",
    "#test$BC33=log(abs(test$BC33+.01))\n",
    "#test$BC98=log(abs(test$BC98+.01))\n",
    "#test$MT28=log(abs(test$MT28+.01))\n",
    "#test$MT36=log(abs(test$MT36+.01))\n",
    "#test$RE34=log(abs(test$RE34+.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the bad bois\n",
    "train$S063 <- NULL\n",
    "train$MT26 <- NULL\n",
    "train$G096 <- NULL\n",
    "train$G091 <- NULL\n",
    "train$G093 <- NULL\n",
    "train$G068 <- NULL\n",
    "train$COLL <- NULL\n",
    "train$BC33 <- NULL\n",
    "train$BC98 <- NULL\n",
    "train$AT33 <- NULL\n",
    "\n",
    "test$S063 <- NULL\n",
    "test$MT26 <- NULL\n",
    "test$G096 <- NULL\n",
    "test$G091 <- NULL\n",
    "test$G093 <- NULL\n",
    "test$G068 <- NULL\n",
    "test$COLL <- NULL\n",
    "test$BC33 <- NULL\n",
    "test$BC98 <- NULL\n",
    "test$AT33 <- NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To install your API key for use in future sessions, run this function with `install = TRUE`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Census Wrangling\n",
    "\n",
    "census_api_key(\"0e29c36736630228c3e7f39191458ccad51de0e1\")\n",
    "v17 <- load_variables(2017, \"acs5\", cache = TRUE)\n",
    "#View(v17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if(FALSE){\n",
    "    \n",
    "# median household income\n",
    "z <- get_acs(geography = \"zcta\", variables = c(houseIncome=\"B19013_001\")\n",
    "z$Zip_Code = z$GEOCODE\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$estimate[is.na(train$estimate)]=median(train$estimate[!is.na(train$estimate)])\n",
    "train$houseIncome = train$estimate\n",
    "\n",
    "#Age\n",
    "z_2 <- get_acs(geography = \"zcta\", variables = \"B01002_001E\")\n",
    "#Income\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B06011_001\")\n",
    "#num males\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01001_002\")\n",
    "head(z_3)\n",
    "#num females\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01001_026\")\n",
    "head(z_3)\n",
    "#median age\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01002_001\")\n",
    "head(z_3)\n",
    "#median age male\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01002_002\")\n",
    "head(z_3)\n",
    "#median age female\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01002_003\")\n",
    "head(z_3)\n",
    "\n",
    "             \n",
    "             \n",
    "#Car drove alone\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B08101_009\")\n",
    "head(z_3)\n",
    "#Carpooled\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B08101_017\")\n",
    "head(z_3)\n",
    "#Public Transportation\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B08006_008\")\n",
    "head(z_3)\n",
    "#Num house with mortgages\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B25027_002\")\n",
    "head(z_3)\n",
    "#Num house without mortgages\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B25027_010\")\n",
    "head(z_3)\n",
    "#Mortgage status\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B25081_001\")\n",
    "head(z_3)\n",
    "             \n",
    "#male 25-34 bachelors degree\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15001_017\")\n",
    "head(z_3)\n",
    "#female 25-34 bachelors degree\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15001_058\")\n",
    "head(z_3)\n",
    "#total bachelor degrees\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15011_001\")\n",
    "head(z_3)\n",
    "#high school diploma\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15003_017\")\n",
    "head(z_3)\n",
    "#less than high school\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B16010_002\")\n",
    "head(z_3)\n",
    "#total masters\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15003_023\")\n",
    "head(z_3)\n",
    "#total doctorate\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15003_025\")\n",
    "head(z_3)\n",
    "             \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#HouseHold Income\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B19013_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,houseIncome = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "hincome = z\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$houseIncome[is.na(train$houseIncome)]=median(train$houseIncome[!is.na(train$houseIncome)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$houseIncome[is.na(test$houseIncome)]=median(test$houseIncome[!is.na(test$houseIncome)])\n",
    "\n",
    "#Age\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01002_001E\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,age = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$age[is.na(train$age)]=median(train$age[!is.na(train$age)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$age[is.na(test$age)]=median(test$age[!is.na(test$age)])\n",
    "\n",
    "#Income\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B06011_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,singIncome = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$singIncome[is.na(train$singIncome)]=median(train$singIncome[!is.na(train$singIncome)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$singIncome[is.na(test$singIncome)]=median(test$singIncome[!is.na(test$singIncome)])\n",
    "\n",
    "#num males\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01001_002\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nMale = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nMale[is.na(train$nMale)]=median(train$nMale[!is.na(train$nMale)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nMale[is.na(test$nMale)]=median(test$nMale[!is.na(test$nMale)])\n",
    "\n",
    "#num femaels\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01001_026\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nFemale = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nFemale[is.na(train$nFemale)]=median(train$nFemale[!is.na(train$nFemale)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nFemale[is.na(test$nFemale)]=median(test$nFemale[!is.na(test$nFemale)])\n",
    "\n",
    "#median age\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01002_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,medAge = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "age = z\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$medAge[is.na(train$medAge)]=median(train$medAge[!is.na(train$medAge)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$medAge[is.na(test$medAge)]=median(test$medAge[!is.na(test$medAge)])\n",
    "\n",
    "#med Age male\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01002_002\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,medAgeMale = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$medAgeMale[is.na(train$medAgeMale)]=median(train$medAgeMale[!is.na(train$medAgeMale)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$medAgeMale[is.na(test$medAgeMale)]=median(test$medAgeMale[!is.na(test$medAgeMale)])\n",
    "\n",
    "#median age femal\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01002_003\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,medAgeFemale = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$medAgeFemale[is.na(train$medAgeFemale)]=median(train$medAgeFemale[!is.na(train$medAgeFemale)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$medAgeFemale[is.na(test$medAgeFemale)]=median(test$medAgeFemale[!is.na(test$medAgeFemale)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Population information - demographics\n",
    "#white\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01001A_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,white = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "hincome = z\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$white[is.na(train$white)]=median(train$white[!is.na(train$white)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$white[is.na(test$white)]=median(test$white[!is.na(test$white)])\n",
    "\n",
    "#black\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01001B_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,black = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$black[is.na(train$black)]=median(train$black[!is.na(train$black)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$black[is.na(test$black)]=median(test$black[!is.na(test$black)])\n",
    "\n",
    "#native americans\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01001C_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,natAm = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$natAm[is.na(train$natAm)]=median(train$natAm[!is.na(train$natAm)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$natAm[is.na(test$natAm)]=median(test$natAm[!is.na(test$natAm)])\n",
    "\n",
    "#asian\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01001F_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,asian = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$asian[is.na(train$asian)]=median(train$asian[!is.na(train$asian)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$asian[is.na(test$asian)]=median(test$asian[!is.na(test$asian)])\n",
    "\n",
    "#hispanic\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01001I_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,hisp = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$hisp[is.na(train$hisp)]=median(train$hisp[!is.na(train$hisp)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$hisp[is.na(test$hisp)]=median(test$hisp[!is.na(test$hisp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Car drove alone\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B08101_009\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,cAlone = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$cAlone[is.na(train$cAlone)]=median(train$cAlone[!is.na(train$cAlone)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$cAlone[is.na(test$cAlone)]=median(test$cAlone[!is.na(test$cAlone)])\n",
    "\n",
    "#Carpooled\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B08101_017\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,cPool = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$cPool[is.na(train$cPool)]=median(train$cPool[!is.na(train$cPool)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$cPool[is.na(test$cPool)]=median(test$cPool[!is.na(test$cPool)])\n",
    "\n",
    "#public transportation\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B08006_008\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,cPub = estimate)\n",
    "pub = z\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$cPub[is.na(train$cPub)]=median(train$cPub[!is.na(train$cPub)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$cPub[is.na(test$cPub)]=median(test$cPub[!is.na(test$cPub)])\n",
    "\n",
    "\n",
    "#num Mortgages\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B25027_002\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nMort = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "mort = z\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nMort[is.na(train$nMort)]=median(train$nMort[!is.na(train$nMort)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nMort[is.na(test$nMort)]=median(test$nMort[!is.na(test$nMort)])\n",
    "\n",
    "#num no Mortgages\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B25027_010\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nNoMort = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nNoMort[is.na(train$nNoMort)]=median(train$nNoMort[!is.na(train$nNoMort)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nNoMort[is.na(test$nNoMort)]=median(test$nNoMort[!is.na(test$nNoMort)])\n",
    "\n",
    "#Mortgage Status\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B25081_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,mortStat = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$mortStat[is.na(train$mortStat)]=median(train$mortStat[!is.na(train$mortStat)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$mortStat[is.na(test$mortStat)]=median(test$mortStat[!is.na(test$mortStat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#male 25-34 bachelors degree\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15001_017\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,mBach = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$mBach[is.na(train$mBach)]=median(train$mBach[!is.na(train$mBach)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$mBach[is.na(test$mBach)]=median(test$mBach[!is.na(test$mBach)])\n",
    "\n",
    "#female 25-34 bachelors degree\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15001_058\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,fBach = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$fBach[is.na(train$fBach)]=median(train$fBach[!is.na(train$fBach)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$fBach[is.na(test$fBach)]=median(test$fBach[!is.na(test$fBach)])\n",
    "\n",
    "#total Bachelors degrees\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15011_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,totBach = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$totBach[is.na(train$totBach)]=median(train$totBach[!is.na(train$totBach)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$totBach[is.na(test$totBach)]=median(test$totBach[!is.na(test$totBach)])\n",
    "\n",
    "#high school diploma\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15003_017\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,hSchool = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "skool = z\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$hSchool[is.na(train$hSchool)]=median(train$hSchool[!is.na(train$hSchool)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$hSchool[is.na(test$hSchool)]=median(test$hSchool[!is.na(test$hSchool)])\n",
    "\n",
    "#less than high school\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B16010_002\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,noSchool = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$noSchool[is.na(train$noSchool)]=median(train$noSchool[!is.na(train$noSchool)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$noSchool[is.na(test$noSchool)]=median(test$noSchool[!is.na(test$noSchool)])\n",
    "\n",
    "#total masters\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15003_023\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,master = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$master[is.na(train$master)]=median(train$master[!is.na(train$master)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$master[is.na(test$master)]=median(test$master[!is.na(test$master)])\n",
    "\n",
    "#total doctorate\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15003_025\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,doc = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$doc[is.na(train$doc)]=median(train$doc[!is.na(train$doc)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$doc[is.na(test$doc)]=median(test$doc[!is.na(test$doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Zip_Code</th><th scope=col>nHouse</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>601</td><td> 600</td></tr>\n",
       "\t<tr><td>602</td><td>1100</td></tr>\n",
       "\t<tr><td>603</td><td>1700</td></tr>\n",
       "\t<tr><td>606</td><td> 250</td></tr>\n",
       "\t<tr><td>610</td><td> 800</td></tr>\n",
       "\t<tr><td>612</td><td>1900</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Zip\\_Code & nHouse\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 601 &  600\\\\\n",
       "\t 602 & 1100\\\\\n",
       "\t 603 & 1700\\\\\n",
       "\t 606 &  250\\\\\n",
       "\t 610 &  800\\\\\n",
       "\t 612 & 1900\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 2\n",
       "\n",
       "| Zip_Code &lt;dbl&gt; | nHouse &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 601 |  600 |\n",
       "| 602 | 1100 |\n",
       "| 603 | 1700 |\n",
       "| 606 |  250 |\n",
       "| 610 |  800 |\n",
       "| 612 | 1900 |\n",
       "\n"
      ],
      "text/plain": [
       "  Zip_Code nHouse\n",
       "1 601       600  \n",
       "2 602      1100  \n",
       "3 603      1700  \n",
       "4 606       250  \n",
       "5 610       800  \n",
       "6 612      1900  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#total housing units\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B00002_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nHouse = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "head(z)\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nHouse[is.na(train$nHouse)]=median(train$nHouse[!is.na(train$nHouse)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nHouse[is.na(test$nHouse)]=median(test$nHouse[!is.na(test$nHouse)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Zip_Code       CreditScore     TYCornUnits    TYBeanUnits     \n",
       " Min.   :  2139   Min.   :  0.0   Min.   :   0   Min.   :    0.0  \n",
       " 1st Qu.: 45846   1st Qu.:733.0   1st Qu.:  97   1st Qu.:  330.0  \n",
       " Median : 47987   Median :785.0   Median : 160   Median :  495.0  \n",
       " Mean   : 51649   Mean   :766.9   Mean   : 231   Mean   :  688.6  \n",
       " 3rd Qu.: 61272   3rd Qu.:817.0   3rd Qu.: 248   3rd Qu.:  720.0  \n",
       " Max.   :475211   Max.   :850.0   Max.   :5940   Max.   :16335.0  \n",
       "  TYWheatUnits     LYCornUnits      LYBeanUnits       LYWheatUnits   \n",
       " Min.   :   0.0   Min.   :   0.0   Min.   :    0.0   Min.   :   0.0  \n",
       " 1st Qu.: 135.0   1st Qu.:  54.0   1st Qu.:  237.0   1st Qu.: 145.0  \n",
       " Median : 135.0   Median : 108.0   Median :  450.0   Median : 145.0  \n",
       " Mean   : 151.1   Mean   : 181.5   Mean   :  718.4   Mean   : 165.1  \n",
       " 3rd Qu.: 135.0   3rd Qu.: 202.0   3rd Qu.:  810.0   3rd Qu.: 145.0  \n",
       " Max.   :4850.0   Max.   :4616.0   Max.   :18303.0   Max.   :4230.0  \n",
       "    State                AT28               AT36             G094         \n",
       " Length:99517       Min.   :       0   Min.   :  0.00   Min.   :0.000000  \n",
       " Class :character   1st Qu.:   41349   1st Qu.: 14.00   1st Qu.:0.000000  \n",
       " Mode  :character   Median :  141800   Median : 14.00   Median :0.000000  \n",
       "                    Mean   :  373136   Mean   : 15.89   Mean   :0.004331  \n",
       "                    3rd Qu.:  380500   3rd Qu.: 14.00   3rd Qu.:0.000000  \n",
       "                    Max.   :11441355   Max.   :182.00   Max.   :5.000000  \n",
       "      MT28              MT36            RE34           PastDue       \n",
       " Min.   :      0   Min.   : 0.00   Min.   :  0.00   Min.   :0.00000  \n",
       " 1st Qu.:      0   1st Qu.:19.00   1st Qu.:  4.70   1st Qu.:0.00000  \n",
       " Median :      0   Median :19.00   Median :  9.60   Median :0.00000  \n",
       " Mean   : 216449   Mean   :19.31   Mean   : 17.45   Mean   :0.03895  \n",
       " 3rd Qu.: 187000   3rd Qu.:19.00   3rd Qu.: 18.80   3rd Qu.:0.00000  \n",
       " Max.   :9999999   Max.   :83.00   Max.   :235.30   Max.   :1.00000  \n",
       "       id         houseIncome          age          singIncome    \n",
       " Min.   :    1   Min.   : 16354   Min.   :12.90   Min.   :  8622  \n",
       " 1st Qu.:24880   1st Qu.: 48419   1st Qu.:37.80   1st Qu.: 25853  \n",
       " Median :49759   Median : 55417   Median :41.00   Median : 28977  \n",
       " Mean   :49759   Mean   : 56870   Mean   :40.85   Mean   : 28928  \n",
       " 3rd Qu.:74638   3rd Qu.: 64531   3rd Qu.:44.10   3rd Qu.: 31220  \n",
       " Max.   :99517   Max.   :216042   Max.   :73.50   Max.   :105452  \n",
       "     nMale          nFemale          medAge        medAgeMale   \n",
       " Min.   :    0   Min.   :    0   Min.   :12.90   Min.   :10.70  \n",
       " 1st Qu.:  899   1st Qu.:  878   1st Qu.:37.80   1st Qu.:36.50  \n",
       " Median : 2203   Median : 2094   Median :41.00   Median :39.70  \n",
       " Mean   : 4251   Mean   : 4303   Mean   :40.85   Mean   :39.73  \n",
       " 3rd Qu.: 5378   3rd Qu.: 5506   3rd Qu.:44.10   3rd Qu.:43.00  \n",
       " Max.   :40568   Max.   :40724   Max.   :73.50   Max.   :72.50  \n",
       "  medAgeFemale       white           black           natAm        \n",
       " Min.   :14.30   Min.   :    0   Min.   :    0   Min.   :   0.00  \n",
       " 1st Qu.:38.60   1st Qu.: 1705   1st Qu.:    2   1st Qu.:   0.00  \n",
       " Median :42.30   Median : 4242   Median :   17   Median :   2.00  \n",
       " Mean   :42.01   Mean   : 7766   Mean   :  364   Mean   :  23.79  \n",
       " 3rd Qu.:45.50   3rd Qu.:10005   3rd Qu.:  138   3rd Qu.:  22.00  \n",
       " Max.   :73.50   Max.   :69780   Max.   :21594   Max.   :3186.00  \n",
       "     asian             hisp             cAlone          cPool     \n",
       " Min.   :   0.0   Min.   :    0.0   Min.   :    0   Min.   :   0  \n",
       " 1st Qu.:   0.0   1st Qu.:   23.0   1st Qu.:  678   1st Qu.:  61  \n",
       " Median :   8.0   Median :  101.0   Median : 1829   Median : 127  \n",
       " Mean   : 102.2   Mean   :  386.5   Mean   : 3304   Mean   : 337  \n",
       " 3rd Qu.:  65.0   3rd Qu.:  325.0   3rd Qu.: 4060   3rd Qu.: 407  \n",
       " Max.   :7125.0   Max.   :18747.0   Max.   :36432   Max.   :3853  \n",
       "      cPub             nMort          nNoMort           mortStat    \n",
       " Min.   :   0.00   Min.   :    0   Min.   :    0.0   Min.   :    0  \n",
       " 1st Qu.:   0.00   1st Qu.:  297   1st Qu.:  254.0   1st Qu.:  567  \n",
       " Median :   1.00   Median :  654   Median :  591.0   Median : 1273  \n",
       " Mean   :  22.82   Mean   : 1425   Mean   :  940.9   Mean   : 2366  \n",
       " 3rd Qu.:  11.00   3rd Qu.: 1794   3rd Qu.: 1263.0   3rd Qu.: 3091  \n",
       " Max.   :7250.00   Max.   :16534   Max.   :18201.0   Max.   :28919  \n",
       "     mBach             fBach           totBach         hSchool     \n",
       " Min.   :   0.00   Min.   :   0.0   Min.   :    0   Min.   :    0  \n",
       " 1st Qu.:  10.00   1st Qu.:  16.0   1st Qu.:  199   1st Qu.:  427  \n",
       " Median :  33.00   Median :  35.0   Median :  406   Median : 1101  \n",
       " Mean   :  86.47   Mean   : 107.2   Mean   : 1272   Mean   : 1796  \n",
       " 3rd Qu.:  81.00   3rd Qu.: 112.0   3rd Qu.: 1436   3rd Qu.: 2227  \n",
       " Max.   :2832.00   Max.   :2471.0   Max.   :28974   Max.   :14957  \n",
       "    noSchool          master             doc              nHouse      \n",
       " Min.   :   0.0   Min.   :    0.0   Min.   :   0.00   Min.   :   0.0  \n",
       " 1st Qu.: 104.0   1st Qu.:   46.0   1st Qu.:   0.00   1st Qu.: 150.0  \n",
       " Median : 225.0   Median :  129.0   Median :   8.00   Median : 200.0  \n",
       " Mean   : 606.1   Mean   :  350.4   Mean   :  48.16   Mean   : 346.4  \n",
       " 3rd Qu.: 729.0   3rd Qu.:  370.0   3rd Qu.:  36.00   3rd Qu.: 450.0  \n",
       " Max.   :6794.0   Max.   :10756.0   Max.   :3632.00   Max.   :2600.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>99517</li>\n",
       "\t<li>44</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 99517\n",
       "\\item 44\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 99517\n",
       "2. 44\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 99517    44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>99517</li>\n",
       "\t<li>49</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 99517\n",
       "\\item 49\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 99517\n",
       "2. 49\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 99517    49"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(zipcode)\n",
    "#zipcode\n",
    "\n",
    "zipcode$Zip_Code = as.numeric(zipcode$zip)\n",
    "\n",
    "dim(train)\n",
    "train = merge(x=train,y=zipcode,by = \"Zip_Code\",all.x=TRUE)\n",
    "test = merge(x=test,y=zipcode,by = \"Zip_Code\",all.x=TRUE)\n",
    "#Make sure I did that rite\n",
    "dim(train)\n",
    "\n",
    "#We get some null lats + longs - fix it\n",
    "\n",
    "train$latitude[is.na(train$latitude)]=median(train$latitude[!is.na(train$latitude)])\n",
    "train$longitude[is.na(train$longitude)]=median(train$longitude[!is.na(train$longitude)])\n",
    "test$latitude[is.na(test$latitude)]=median(test$latitude[!is.na(test$latitude)])\n",
    "test$longitude[is.na(test$longitude)]=median(test$longitude[!is.na(test$longitude)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the population info\n",
    "train = transform(train, pop = nMale + nFemale)\n",
    "test = transform(test, pop = nMale + nFemale)\n",
    "#train$pop[is.zero(train$pop)]=median(train$pop[!is.zero(train$pop)])\n",
    "#test$pop[is.zero(test$pop)]=median(test$pop[!is.zero(test$pop)])\n",
    "\n",
    "train = transform(train, pnMale = nMale/pop)\n",
    "test = transform(test, pnMale = nMale/pop)\n",
    "train$pnMale[is.na(train$pnMale)]=median(train$pnMale[!is.na(train$pnMale)])\n",
    "test$pnMale[is.na(test$pnMale)]=median(test$pnMale[!is.na(test$pnMale)])\n",
    "train = transform(train, pnFemale = nFemale/pop)\n",
    "test = transform(test, pnFemale = nFemale/pop)\n",
    "train$pnFemale[is.na(train$pnFemale)]=median(train$pnFemale[!is.na(train$pnFemale)])\n",
    "test$pnFemale[is.na(test$pnFemale)]=median(test$pnFemale[!is.na(test$pnFemale)])\n",
    "#Demographics\n",
    "train = transform(train, pwhite = white/pop)\n",
    "test = transform(test, pwhite = white/pop)\n",
    "train$pwhite[is.na(train$pwhite)]=median(train$pwhite[!is.na(train$pwhite)])\n",
    "test$pwhite[is.na(test$pwhite)]=median(test$pwhite[!is.na(test$pwhite)])\n",
    "train = transform(train, pblack = black/pop)\n",
    "test = transform(test, pblack = black/pop)\n",
    "train$pblack[is.na(train$pblack)]=median(train$pblack[!is.na(train$pblack)])\n",
    "test$pblack[is.na(test$pblack)]=median(test$pblack[!is.na(test$pblack)])\n",
    "train = transform(train, pnatAm = natAm/pop)\n",
    "test = transform(test, pnatAm = natAm/pop)\n",
    "train$pnatAm[is.na(train$pnatAm)]=median(train$pnatAm[!is.na(train$pnatAm)])\n",
    "test$pnatAm[is.na(test$pnatAm)]=median(test$pnatAm[!is.na(test$pnatAm)])\n",
    "train = transform(train, pasian = asian/pop)\n",
    "test = transform(test, pasian = asian/pop)\n",
    "train$pasian[is.na(train$pasian)]=median(train$pasian[!is.na(train$pasian)])\n",
    "test$pasian[is.na(test$pasian)]=median(test$pasian[!is.na(test$pasian)])\n",
    "train = transform(train, phisp = hisp/pop)\n",
    "test = transform(test, phisp = hisp/pop)\n",
    "train$phisp[is.na(train$phisp)]=median(train$phisp[!is.na(train$phisp)])\n",
    "test$phisp[is.na(test$phisp)]=median(test$phisp[!is.na(test$phisp)])\n",
    "#Transport\n",
    "train = transform(train, pcAlone = cAlone/pop)\n",
    "test = transform(test, pcAlone = cAlone/pop)\n",
    "train$pcAlone[is.na(train$pcAlone)]=median(train$pcAlone[!is.na(train$pcAlone)])\n",
    "test$pcAlone[is.na(test$pcAlone)]=median(test$pcAlone[!is.na(test$pcAlone)])\n",
    "train = transform(train, pcPool = cPool/pop)\n",
    "test = transform(test, pcPool = cPool/pop)\n",
    "train$pcPool[is.na(train$pcPool)]=median(train$pcPool[!is.na(train$pcPool)])\n",
    "test$pcPool[is.na(test$pcPool)]=median(test$pcPool[!is.na(test$pcPool)])\n",
    "train = transform(train, pcPub = cPub/pop)\n",
    "test = transform(test, pcPub = cPub/pop)\n",
    "train$pcPub[is.na(train$pcPub)]=median(train$pcPub[!is.na(train$pcPub)])\n",
    "test$pcPub[is.na(test$pcPub)]=median(test$pcPub[!is.na(test$pcPub)])\n",
    "#Education\n",
    "train = transform(train, pmBach = mBach/pop)\n",
    "test = transform(test, pmBach = mBach/pop)\n",
    "train$pmBach[is.na(train$pmBach)]=median(train$pmBach[!is.na(train$pmBach)])\n",
    "test$pmBach[is.na(test$pmBach)]=median(test$pmBach[!is.na(test$pmBach)])\n",
    "train = transform(train, pfBach = fBach/pop)\n",
    "test = transform(test, pfBach = fBach/pop)\n",
    "train$pfBach[is.na(train$pfBach)]=median(train$pfBach[!is.na(train$pfBach)])\n",
    "test$pfBach[is.na(test$pfBach)]=median(test$pfBach[!is.na(test$pfBach)])\n",
    "train = transform(train, ptotBach = totBach/pop)\n",
    "test = transform(test, ptotBach = totBach/pop)\n",
    "train$ptotBach[is.na(train$ptotBach)]=median(train$ptotBach[!is.na(train$ptotBach)])\n",
    "test$ptotBach[is.na(test$ptotBach)]=median(test$ptotBach[!is.na(test$ptotBach)])\n",
    "train = transform(train, phSchool = hSchool/pop)\n",
    "test = transform(test, phSchool = hSchool/pop)\n",
    "train$phSchool[is.na(train$phSchool)]=median(train$phSchool[!is.na(train$phSchool)])\n",
    "test$phSchool[is.na(test$phSchool)]=median(test$phSchool[!is.na(test$phSchool)])\n",
    "train = transform(train, pnoSchool = noSchool/pop)\n",
    "test = transform(test, pnoSchool = noSchool/pop)\n",
    "train$pnoSchool[is.na(train$pnoSchool)]=median(train$pnoSchool[!is.na(train$pnoSchool)])\n",
    "test$pnoSchool[is.na(test$pnoSchool)]=median(test$pnoSchool[!is.na(test$pnoSchool)])\n",
    "train = transform(train, pmaster = master/pop)\n",
    "test = transform(test, pmaster = master/pop)\n",
    "train$pmaster[is.na(train$pmaster)]=median(train$pmaster[!is.na(train$pmaster)])\n",
    "test$pmaster[is.na(test$pmaster)]=median(test$pmaster[!is.na(test$pmaster)])\n",
    "train = transform(train, pdoc = doc/pop)\n",
    "test = transform(test, pdoc = doc/pop)\n",
    "train$pdoc[is.na(train$pdoc)]=median(train$pdoc[!is.na(train$pdoc)])\n",
    "test$pdoc[is.na(test$pdoc)]=median(test$pdoc[!is.na(test$pdoc)])\n",
    "\n",
    "\n",
    "#Housing information\n",
    "train = transform(train, pnMort = nMort/nHouse)\n",
    "test = transform(test, pnMort = nMort/nHouse)\n",
    "train$pnMort[is.na(train$pnMort)]=median(train$pnMort[!is.na(train$pnMort)])\n",
    "test$pnMort[is.na(test$pnMort)]=median(test$pnMort[!is.na(test$pnMort)])\n",
    "train = transform(train, pnNoMort = nNoMort/nHouse)\n",
    "test = transform(test, pnNoMort = nNoMort/nHouse)\n",
    "train$pnNoMort[is.na(train$pnNoMort)]=median(train$pnNoMort[!is.na(train$pnNoMort)])\n",
    "test$pnNoMort[is.na(test$pnNoMort)]=median(test$pnNoMort[!is.na(test$pnNoMort)])\n",
    "train = transform(train, pmortStat = mortStat/nHouse)\n",
    "test = transform(test, pmortStat = mortStat/nHouse)\n",
    "train$pmortStat[is.na(train$pmortStat)]=median(train$pmortStat[!is.na(train$pmortStat)])\n",
    "test$pmortStat[is.na(test$pmortStat)]=median(test$pmortStat[!is.na(test$pmortStat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  CreditScore     TYCornUnits    TYBeanUnits       TYWheatUnits   \n",
       " Min.   :  0.0   Min.   :   0   Min.   :    0.0   Min.   :   0.0  \n",
       " 1st Qu.:733.0   1st Qu.:  97   1st Qu.:  330.0   1st Qu.: 135.0  \n",
       " Median :785.0   Median : 160   Median :  495.0   Median : 135.0  \n",
       " Mean   :766.9   Mean   : 231   Mean   :  688.6   Mean   : 151.1  \n",
       " 3rd Qu.:817.0   3rd Qu.: 248   3rd Qu.:  720.0   3rd Qu.: 135.0  \n",
       " Max.   :850.0   Max.   :5940   Max.   :16335.0   Max.   :4850.0  \n",
       "  LYCornUnits      LYBeanUnits       LYWheatUnits       State          \n",
       " Min.   :   0.0   Min.   :    0.0   Min.   :   0.0   Length:99517      \n",
       " 1st Qu.:  54.0   1st Qu.:  237.0   1st Qu.: 145.0   Class :character  \n",
       " Median : 108.0   Median :  450.0   Median : 145.0   Mode  :character  \n",
       " Mean   : 181.5   Mean   :  718.4   Mean   : 165.1                     \n",
       " 3rd Qu.: 202.0   3rd Qu.:  810.0   3rd Qu.: 145.0                     \n",
       " Max.   :4616.0   Max.   :18303.0   Max.   :4230.0                     \n",
       "      AT28               AT36             G094               MT28        \n",
       " Min.   :       0   Min.   :  0.00   Min.   :0.000000   Min.   :      0  \n",
       " 1st Qu.:   41349   1st Qu.: 14.00   1st Qu.:0.000000   1st Qu.:      0  \n",
       " Median :  141800   Median : 14.00   Median :0.000000   Median :      0  \n",
       " Mean   :  373136   Mean   : 15.89   Mean   :0.004331   Mean   : 216449  \n",
       " 3rd Qu.:  380500   3rd Qu.: 14.00   3rd Qu.:0.000000   3rd Qu.: 187000  \n",
       " Max.   :11441355   Max.   :182.00   Max.   :5.000000   Max.   :9999999  \n",
       "      MT36            RE34           PastDue              id       \n",
       " Min.   : 0.00   Min.   :  0.00   Min.   :0.00000   Min.   :    1  \n",
       " 1st Qu.:19.00   1st Qu.:  4.70   1st Qu.:0.00000   1st Qu.:24880  \n",
       " Median :19.00   Median :  9.60   Median :0.00000   Median :49759  \n",
       " Mean   :19.31   Mean   : 17.45   Mean   :0.03895   Mean   :49759  \n",
       " 3rd Qu.:19.00   3rd Qu.: 18.80   3rd Qu.:0.00000   3rd Qu.:74638  \n",
       " Max.   :83.00   Max.   :235.30   Max.   :1.00000   Max.   :99517  \n",
       "  houseIncome          age          singIncome         nMale      \n",
       " Min.   : 16354   Min.   :12.90   Min.   :  8622   Min.   :    0  \n",
       " 1st Qu.: 48419   1st Qu.:37.80   1st Qu.: 25853   1st Qu.:  899  \n",
       " Median : 55417   Median :41.00   Median : 28977   Median : 2203  \n",
       " Mean   : 56870   Mean   :40.85   Mean   : 28928   Mean   : 4251  \n",
       " 3rd Qu.: 64531   3rd Qu.:44.10   3rd Qu.: 31220   3rd Qu.: 5378  \n",
       " Max.   :216042   Max.   :73.50   Max.   :105452   Max.   :40568  \n",
       "    nFemale          medAge        medAgeMale     medAgeFemale  \n",
       " Min.   :    0   Min.   :12.90   Min.   :10.70   Min.   :14.30  \n",
       " 1st Qu.:  878   1st Qu.:37.80   1st Qu.:36.50   1st Qu.:38.60  \n",
       " Median : 2094   Median :41.00   Median :39.70   Median :42.30  \n",
       " Mean   : 4303   Mean   :40.85   Mean   :39.73   Mean   :42.01  \n",
       " 3rd Qu.: 5506   3rd Qu.:44.10   3rd Qu.:43.00   3rd Qu.:45.50  \n",
       " Max.   :40724   Max.   :73.50   Max.   :72.50   Max.   :73.50  \n",
       "     white           black           natAm             asian       \n",
       " Min.   :    0   Min.   :    0   Min.   :   0.00   Min.   :   0.0  \n",
       " 1st Qu.: 1705   1st Qu.:    2   1st Qu.:   0.00   1st Qu.:   0.0  \n",
       " Median : 4242   Median :   17   Median :   2.00   Median :   8.0  \n",
       " Mean   : 7766   Mean   :  364   Mean   :  23.79   Mean   : 102.2  \n",
       " 3rd Qu.:10005   3rd Qu.:  138   3rd Qu.:  22.00   3rd Qu.:  65.0  \n",
       " Max.   :69780   Max.   :21594   Max.   :3186.00   Max.   :7125.0  \n",
       "      hisp             cAlone          cPool           cPub        \n",
       " Min.   :    0.0   Min.   :    0   Min.   :   0   Min.   :   0.00  \n",
       " 1st Qu.:   23.0   1st Qu.:  678   1st Qu.:  61   1st Qu.:   0.00  \n",
       " Median :  101.0   Median : 1829   Median : 127   Median :   1.00  \n",
       " Mean   :  386.5   Mean   : 3304   Mean   : 337   Mean   :  22.82  \n",
       " 3rd Qu.:  325.0   3rd Qu.: 4060   3rd Qu.: 407   3rd Qu.:  11.00  \n",
       " Max.   :18747.0   Max.   :36432   Max.   :3853   Max.   :7250.00  \n",
       "     nMort          nNoMort           mortStat         mBach        \n",
       " Min.   :    0   Min.   :    0.0   Min.   :    0   Min.   :   0.00  \n",
       " 1st Qu.:  297   1st Qu.:  254.0   1st Qu.:  567   1st Qu.:  10.00  \n",
       " Median :  654   Median :  591.0   Median : 1273   Median :  33.00  \n",
       " Mean   : 1425   Mean   :  940.9   Mean   : 2366   Mean   :  86.47  \n",
       " 3rd Qu.: 1794   3rd Qu.: 1263.0   3rd Qu.: 3091   3rd Qu.:  81.00  \n",
       " Max.   :16534   Max.   :18201.0   Max.   :28919   Max.   :2832.00  \n",
       "     fBach           totBach         hSchool         noSchool     \n",
       " Min.   :   0.0   Min.   :    0   Min.   :    0   Min.   :   0.0  \n",
       " 1st Qu.:  16.0   1st Qu.:  199   1st Qu.:  427   1st Qu.: 104.0  \n",
       " Median :  35.0   Median :  406   Median : 1101   Median : 225.0  \n",
       " Mean   : 107.2   Mean   : 1272   Mean   : 1796   Mean   : 606.1  \n",
       " 3rd Qu.: 112.0   3rd Qu.: 1436   3rd Qu.: 2227   3rd Qu.: 729.0  \n",
       " Max.   :2471.0   Max.   :28974   Max.   :14957   Max.   :6794.0  \n",
       "     master             doc              nHouse          latitude    \n",
       " Min.   :    0.0   Min.   :   0.00   Min.   :   0.0   Min.   :25.56  \n",
       " 1st Qu.:   46.0   1st Qu.:   0.00   1st Qu.: 150.0   1st Qu.:39.19  \n",
       " Median :  129.0   Median :   8.00   Median : 200.0   Median :40.41  \n",
       " Mean   :  350.4   Mean   :  48.16   Mean   : 346.4   Mean   :40.25  \n",
       " 3rd Qu.:  370.0   3rd Qu.:  36.00   3rd Qu.: 450.0   3rd Qu.:41.32  \n",
       " Max.   :10756.0   Max.   :3632.00   Max.   :2600.0   Max.   :47.44  \n",
       "   longitude            pop            pnMale          pnFemale     \n",
       " Min.   :-123.17   Min.   :    0   Min.   :0.3212   Min.   :0.1050  \n",
       " 1st Qu.: -90.05   1st Qu.: 1761   1st Qu.:0.4861   1st Qu.:0.4873  \n",
       " Median : -87.48   Median : 4297   Median :0.4990   Median :0.5010  \n",
       " Mean   : -88.05   Mean   : 8554   Mean   :0.5014   Mean   :0.4986  \n",
       " 3rd Qu.: -85.00   3rd Qu.:10769   3rd Qu.:0.5127   3rd Qu.:0.5139  \n",
       " Max.   : -71.10   Max.   :80489   Max.   :0.8950   Max.   :0.6788  \n",
       "     pwhite           pblack              pnatAm              pasian        \n",
       " Min.   :0.1378   Min.   :0.0000000   Min.   :0.0000000   Min.   :0.000000  \n",
       " 1st Qu.:0.9342   1st Qu.:0.0006882   1st Qu.:0.0000000   1st Qu.:0.000000  \n",
       " Median :0.9658   Median :0.0044999   Median :0.0004654   Median :0.001899  \n",
       " Mean   :0.9453   Mean   :0.0207881   Mean   :0.0031864   Mean   :0.008208  \n",
       " 3rd Qu.:0.9836   3rd Qu.:0.0170411   3rd Qu.:0.0029018   3rd Qu.:0.007905  \n",
       " Max.   :1.0000   Max.   :0.8233336   Max.   :0.5596346   Max.   :0.770408  \n",
       "     phisp            pcAlone           pcPool            pcPub          \n",
       " Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000000  \n",
       " 1st Qu.:0.01006   1st Qu.:0.3550   1st Qu.:0.02816   1st Qu.:0.0000000  \n",
       " Median :0.02350   Median :0.3908   Median :0.03541   Median :0.0002374  \n",
       " Mean   :0.03351   Mean   :0.3893   Mean   :0.03825   Mean   :0.0015819  \n",
       " 3rd Qu.:0.03791   3rd Qu.:0.4306   3rd Qu.:0.04759   3rd Qu.:0.0018618  \n",
       " Max.   :0.77041   Max.   :0.6986   Max.   :0.32653   Max.   :0.3705416  \n",
       "     pmBach             pfBach            ptotBach          phSchool     \n",
       " Min.   :0.000000   Min.   :0.000000   Min.   :0.00000   Min.   :0.0000  \n",
       " 1st Qu.:0.003841   1st Qu.:0.006319   1st Qu.:0.08862   1st Qu.:0.2031  \n",
       " Median :0.007680   Median :0.009379   Median :0.11681   Median :0.2391  \n",
       " Mean   :0.008408   Mean   :0.010876   Mean   :0.12647   Mean   :0.2369  \n",
       " 3rd Qu.:0.011292   3rd Qu.:0.014383   3rd Qu.:0.14934   3rd Qu.:0.2747  \n",
       " Max.   :0.167798   Max.   :0.115903   Max.   :0.76057   Max.   :1.0000  \n",
       "   pnoSchool          pmaster             pdoc              pnMort      \n",
       " Min.   :0.00000   Min.   :0.00000   Min.   :0.000000   Min.   : 0.000  \n",
       " 1st Qu.:0.04764   1st Qu.:0.02265   1st Qu.:0.000000   1st Qu.: 1.907  \n",
       " Median :0.06296   Median :0.03002   Median :0.001898   Median : 3.270  \n",
       " Mean   :0.07017   Mean   :0.03318   Mean   :0.003422   Mean   : 3.429  \n",
       " 3rd Qu.:0.08767   3rd Qu.:0.03969   3rd Qu.:0.004497   3rd Qu.: 4.485  \n",
       " Max.   :1.00000   Max.   :0.24554   Max.   :0.085885   Max.   :22.240  \n",
       "    pnNoMort        pmortStat     \n",
       " Min.   : 0.000   Min.   : 0.000  \n",
       " 1st Qu.: 1.620   1st Qu.: 3.587  \n",
       " Median : 2.456   Median : 5.958  \n",
       " Mean   : 2.510   Mean   : 5.939  \n",
       " 3rd Qu.: 3.095   3rd Qu.: 7.622  \n",
       " Max.   :19.159   Max.   :30.441  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Remove more bad bois\n",
    "train$state <- NULL\n",
    "train$Zip_Code <- NULL\n",
    "train$zip <- NULL\n",
    "train$city <- NULL\n",
    "\n",
    "test$state <- NULL\n",
    "test$Zip_Code <- NULL\n",
    "test$zip <- NULL\n",
    "test$city <- NULL\n",
    "\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1670</li>\n",
       "\t<li>61510</li>\n",
       "\t<li>50649</li>\n",
       "\t<li>28405</li>\n",
       "\t<li>70796</li>\n",
       "\t<li>3779</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1670\n",
       "\\item 61510\n",
       "\\item 50649\n",
       "\\item 28405\n",
       "\\item 70796\n",
       "\\item 3779\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1670\n",
       "2. 61510\n",
       "3. 50649\n",
       "4. 28405\n",
       "5. 70796\n",
       "6. 3779\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  1670 61510 50649 28405 70796  3779"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 0\n",
       "3. 0\n",
       "4. 0\n",
       "5. 0\n",
       "6. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0 0 0 0 0 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 0\n",
       "3. 0\n",
       "4. 0\n",
       "5. 0\n",
       "6. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0 0 0 0 0 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>11.4</li>\n",
       "\t<li>11.4</li>\n",
       "\t<li>9.6</li>\n",
       "\t<li>9.6</li>\n",
       "\t<li>0</li>\n",
       "\t<li>42.5</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 11.4\n",
       "\\item 11.4\n",
       "\\item 9.6\n",
       "\\item 9.6\n",
       "\\item 0\n",
       "\\item 42.5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 11.4\n",
       "2. 11.4\n",
       "3. 9.6\n",
       "4. 9.6\n",
       "5. 0\n",
       "6. 42.5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 11.4 11.4  9.6  9.6  0.0 42.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(train$id)\n",
    "head(train[,15])\n",
    "head(train$PastDue)\n",
    "head(train[,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in colMeans(x, na.rm = TRUE): 'x' must be numeric\n",
     "output_type": "error",
     "traceback": [
      "Error in colMeans(x, na.rm = TRUE): 'x' must be numeric\nTraceback:\n",
      "1. prcomp(pca_train, center = TRUE, scale = TRUE)",
      "2. prcomp.default(pca_train, center = TRUE, scale = TRUE)",
      "3. scale(x, center = center, scale = scale.)",
      "4. scale.default(x, center = center, scale = scale.)",
      "5. colMeans(x, na.rm = TRUE)"
     ]
    }
   ],
   "source": [
    "#PCA Time\n",
    "pca_train = train\n",
    "pca_train$id <- NULL\n",
    "pca_train$PastDue <- NULL\n",
    "pca <- prcomp(pca_train,center=TRUE,scale=TRUE)\n",
    "train$PCA1 = pca$x[,1]\n",
    "train$PCA2 = pca$x[,2]\n",
    "train$PCA3 = pca$x[,3]\n",
    "train$PCA4 = pca$x[,4]\n",
    "train$PCA5 = pca$x[,5]\n",
    "train$PCA6 = pca$x[,6]\n",
    "train$PCA7 = pca$x[,7]\n",
    "train$PCA8 = pca$x[,8]\n",
    "train$PCA9 = pca$x[,9]\n",
    "train$PCA0 = pca$x[,10]\n",
    "\n",
    "#PCA Time\n",
    "pca_train = test\n",
    "pca_train$id <- NULL\n",
    "pca_train$PastDue <- NULL\n",
    "pca <- prcomp(pca_train,center=TRUE,scale=TRUE)\n",
    "test$PCA1 = pca$x[,1]\n",
    "test$PCA2 = pca$x[,2]\n",
    "test$PCA3 = pca$x[,3]\n",
    "test$PCA4 = pca$x[,4]\n",
    "test$PCA5 = pca$x[,5]\n",
    "test$PCA6 = pca$x[,6]\n",
    "test$PCA7 = pca$x[,7]\n",
    "test$PCA8 = pca$x[,8]\n",
    "test$PCA9 = pca$x[,9]\n",
    "test$PCA0 = pca$x[,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(substitute(list(...)), `_data`, parent.frame()): object 'PCA1' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(substitute(list(...)), `_data`, parent.frame()): object 'PCA1' not found\nTraceback:\n",
      "1. transform(new_train, PCA1 = PCA1 * normy)",
      "2. transform.data.frame(new_train, PCA1 = PCA1 * normy)",
      "3. eval(substitute(list(...)), `_data`, parent.frame())",
      "4. eval(substitute(list(...)), `_data`, parent.frame())"
     ]
    }
   ],
   "source": [
    "new_train = train\n",
    "\n",
    "old_train = train\n",
    "\n",
    "sz = dim(new_train)\n",
    "\n",
    "our_sd = .02\n",
    "\n",
    " \n",
    "\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,CreditScore = CreditScore * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,TYCornUnits = TYCornUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,TYBeanUnits = TYBeanUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,TYWheatUnits = TYWheatUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,LYCornUnits = LYCornUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,LYBeanUnits = LYBeanUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,LYWheatUnits = LYWheatUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,houseIncome = houseIncome * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,age = age * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,singIncome = singIncome * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nMale = nMale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nFemale = nFemale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,medAge = medAge * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,medAgeMale = medAgeMale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,medAgeFemale = medAgeFemale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,cAlone = cAlone * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,cPool = cPool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,cPub = cPub * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nMort = nMort * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,mortStat = mortStat * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nNoMort = nNoMort * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,mBach = mBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,fBach = fBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,totBach = totBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,hSchool = hSchool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,noSchool = noSchool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,master = master * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,doc = doc * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nHouse = nHouse * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,latitude = latitude * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,longitude = longitude * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pop = pop * normy)\n",
    "\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,white = white * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,black = black * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,natAm = natAm * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,asian = asian * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,hisp = hisp * normy)\n",
    "\n",
    " \n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA1 = PCA1 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA2 = PCA2 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA3 = PCA3 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA4 = PCA4 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA5 = PCA5 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA6 = PCA6 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA7 = PCA7 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA8 = PCA8 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA9 = PCA9 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA0 = PCA0 * normy)\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "#Percentages\n",
    "\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnMale = pnMale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnFemale = pnFemale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pcAlone = pcAlone * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pcPool = pcPool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pcPub = pcPub * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnMort = pnMort * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pmortStat = pmortStat * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnNoMort = pnNoMort * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pmBach = pmBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pfBach = pfBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,ptotBach = ptotBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,phSchool = phSchool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnoSchool = pnoSchool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pmaster = pmaster * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pdoc = pdoc * normy)\n",
    "\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pwhite = pwhite * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pblack = pblack * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnatAm = pnatAm * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pasian = pasian * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,phisp = phisp * normy)\n",
    "\n",
    " \n",
    "\n",
    "new_train$normy <- NULL\n",
    "\n",
    " \n",
    "\n",
    "rbind(train,new_train)\n",
    "\n",
    " \n",
    "\n",
    "summary(new_train)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "new_train = old_train[(train$PastDue == 1),]\n",
    "sz = dim(new_train)\n",
    "our_sd = .02\n",
    "\n",
    " \n",
    "\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,CreditScore = CreditScore * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,TYCornUnits = TYCornUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,TYBeanUnits = TYBeanUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,TYWheatUnits = TYWheatUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,LYCornUnits = LYCornUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,LYBeanUnits = LYBeanUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,LYWheatUnits = LYWheatUnits * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,houseIncome = houseIncome * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,age = age * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,singIncome = singIncome * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nMale = nMale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nFemale = nFemale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,medAge = medAge * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,medAgeMale = medAgeMale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,medAgeFemale = medAgeFemale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,cAlone = cAlone * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,cPool = cPool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,cPub = cPub * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nMort = nMort * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,mortStat = mortStat * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nNoMort = nNoMort * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,mBach = mBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,fBach = fBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,totBach = totBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,hSchool = hSchool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,noSchool = noSchool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,master = master * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,doc = doc * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,nHouse = nHouse * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,latitude = latitude * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,longitude = longitude * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pop = pop * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "\n",
    "new_train = transform(new_train,white = white * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,black = black * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,natAm = natAm * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,asian = asian * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,hisp = hisp * normy)\n",
    "\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA1 = PCA1 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA2 = PCA2 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA3 = PCA3 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA4 = PCA4 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA5 = PCA5 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA6 = PCA6 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA7 = PCA7 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA8 = PCA8 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA9 = PCA9 * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,PCA0 = PCA0 * normy)\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "#Percentages\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnMale = pnMale * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnFemale = pnFemale * normy)\n",
    " \n",
    " \n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pcAlone = pcAlone * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pcPool = pcPool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pcPub = pcPub * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnMort = pnMort * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pmortStat = pmortStat * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnNoMort = pnNoMort * normy)\n",
    " \n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pmBach = pmBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pfBach = pfBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,ptotBach = ptotBach * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,phSchool = phSchool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnoSchool = pnoSchool * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pmaster = pmaster * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pdoc = pdoc * normy)\n",
    "\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pwhite = pwhite * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pblack = pblack * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pnatAm = pnatAm * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,pasian = pasian * normy)\n",
    "new_train$normy = rnorm(sz[1],mean=1,sd=our_sd)\n",
    "new_train = transform(new_train,phisp = phisp * normy)\n",
    " \n",
    "\n",
    "new_train$normy <- NULL\n",
    "\n",
    " \n",
    "\n",
    "rbind(train,new_train)\n",
    "\n",
    " \n",
    "\n",
    "summary(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  CreditScore     TYCornUnits    TYBeanUnits       TYWheatUnits   \n",
       " Min.   :  0.0   Min.   :   0   Min.   :    0.0   Min.   :   0.0  \n",
       " 1st Qu.:733.0   1st Qu.:  97   1st Qu.:  330.0   1st Qu.: 135.0  \n",
       " Median :785.0   Median : 160   Median :  495.0   Median : 135.0  \n",
       " Mean   :766.9   Mean   : 231   Mean   :  688.6   Mean   : 151.1  \n",
       " 3rd Qu.:817.0   3rd Qu.: 248   3rd Qu.:  720.0   3rd Qu.: 135.0  \n",
       " Max.   :850.0   Max.   :5940   Max.   :16335.0   Max.   :4850.0  \n",
       "  LYCornUnits      LYBeanUnits       LYWheatUnits       State          \n",
       " Min.   :   0.0   Min.   :    0.0   Min.   :   0.0   Length:99517      \n",
       " 1st Qu.:  54.0   1st Qu.:  237.0   1st Qu.: 145.0   Class :character  \n",
       " Median : 108.0   Median :  450.0   Median : 145.0   Mode  :character  \n",
       " Mean   : 181.5   Mean   :  718.4   Mean   : 165.1                     \n",
       " 3rd Qu.: 202.0   3rd Qu.:  810.0   3rd Qu.: 145.0                     \n",
       " Max.   :4616.0   Max.   :18303.0   Max.   :4230.0                     \n",
       "      AT28               AT36             G094               MT28        \n",
       " Min.   :       0   Min.   :  0.00   Min.   :0.000000   Min.   :      0  \n",
       " 1st Qu.:   41349   1st Qu.: 14.00   1st Qu.:0.000000   1st Qu.:      0  \n",
       " Median :  141800   Median : 14.00   Median :0.000000   Median :      0  \n",
       " Mean   :  373136   Mean   : 15.89   Mean   :0.004331   Mean   : 216449  \n",
       " 3rd Qu.:  380500   3rd Qu.: 14.00   3rd Qu.:0.000000   3rd Qu.: 187000  \n",
       " Max.   :11441355   Max.   :182.00   Max.   :5.000000   Max.   :9999999  \n",
       "      MT36            RE34           PastDue              id       \n",
       " Min.   : 0.00   Min.   :  0.00   Min.   :0.00000   Min.   :    1  \n",
       " 1st Qu.:19.00   1st Qu.:  4.70   1st Qu.:0.00000   1st Qu.:24880  \n",
       " Median :19.00   Median :  9.60   Median :0.00000   Median :49759  \n",
       " Mean   :19.31   Mean   : 17.45   Mean   :0.03895   Mean   :49759  \n",
       " 3rd Qu.:19.00   3rd Qu.: 18.80   3rd Qu.:0.00000   3rd Qu.:74638  \n",
       " Max.   :83.00   Max.   :235.30   Max.   :1.00000   Max.   :99517  \n",
       "  houseIncome          age          singIncome         nMale      \n",
       " Min.   : 16354   Min.   :12.90   Min.   :  8622   Min.   :    0  \n",
       " 1st Qu.: 48419   1st Qu.:37.80   1st Qu.: 25853   1st Qu.:  899  \n",
       " Median : 55417   Median :41.00   Median : 28977   Median : 2203  \n",
       " Mean   : 56870   Mean   :40.85   Mean   : 28928   Mean   : 4251  \n",
       " 3rd Qu.: 64531   3rd Qu.:44.10   3rd Qu.: 31220   3rd Qu.: 5378  \n",
       " Max.   :216042   Max.   :73.50   Max.   :105452   Max.   :40568  \n",
       "    nFemale          medAge        medAgeMale     medAgeFemale  \n",
       " Min.   :    0   Min.   :12.90   Min.   :10.70   Min.   :14.30  \n",
       " 1st Qu.:  878   1st Qu.:37.80   1st Qu.:36.50   1st Qu.:38.60  \n",
       " Median : 2094   Median :41.00   Median :39.70   Median :42.30  \n",
       " Mean   : 4303   Mean   :40.85   Mean   :39.73   Mean   :42.01  \n",
       " 3rd Qu.: 5506   3rd Qu.:44.10   3rd Qu.:43.00   3rd Qu.:45.50  \n",
       " Max.   :40724   Max.   :73.50   Max.   :72.50   Max.   :73.50  \n",
       "     white           black           natAm             asian       \n",
       " Min.   :    0   Min.   :    0   Min.   :   0.00   Min.   :   0.0  \n",
       " 1st Qu.: 1705   1st Qu.:    2   1st Qu.:   0.00   1st Qu.:   0.0  \n",
       " Median : 4242   Median :   17   Median :   2.00   Median :   8.0  \n",
       " Mean   : 7766   Mean   :  364   Mean   :  23.79   Mean   : 102.2  \n",
       " 3rd Qu.:10005   3rd Qu.:  138   3rd Qu.:  22.00   3rd Qu.:  65.0  \n",
       " Max.   :69780   Max.   :21594   Max.   :3186.00   Max.   :7125.0  \n",
       "      hisp             cAlone          cPool           cPub        \n",
       " Min.   :    0.0   Min.   :    0   Min.   :   0   Min.   :   0.00  \n",
       " 1st Qu.:   23.0   1st Qu.:  678   1st Qu.:  61   1st Qu.:   0.00  \n",
       " Median :  101.0   Median : 1829   Median : 127   Median :   1.00  \n",
       " Mean   :  386.5   Mean   : 3304   Mean   : 337   Mean   :  22.82  \n",
       " 3rd Qu.:  325.0   3rd Qu.: 4060   3rd Qu.: 407   3rd Qu.:  11.00  \n",
       " Max.   :18747.0   Max.   :36432   Max.   :3853   Max.   :7250.00  \n",
       "     nMort          nNoMort           mortStat         mBach        \n",
       " Min.   :    0   Min.   :    0.0   Min.   :    0   Min.   :   0.00  \n",
       " 1st Qu.:  297   1st Qu.:  254.0   1st Qu.:  567   1st Qu.:  10.00  \n",
       " Median :  654   Median :  591.0   Median : 1273   Median :  33.00  \n",
       " Mean   : 1425   Mean   :  940.9   Mean   : 2366   Mean   :  86.47  \n",
       " 3rd Qu.: 1794   3rd Qu.: 1263.0   3rd Qu.: 3091   3rd Qu.:  81.00  \n",
       " Max.   :16534   Max.   :18201.0   Max.   :28919   Max.   :2832.00  \n",
       "     fBach           totBach         hSchool         noSchool     \n",
       " Min.   :   0.0   Min.   :    0   Min.   :    0   Min.   :   0.0  \n",
       " 1st Qu.:  16.0   1st Qu.:  199   1st Qu.:  427   1st Qu.: 104.0  \n",
       " Median :  35.0   Median :  406   Median : 1101   Median : 225.0  \n",
       " Mean   : 107.2   Mean   : 1272   Mean   : 1796   Mean   : 606.1  \n",
       " 3rd Qu.: 112.0   3rd Qu.: 1436   3rd Qu.: 2227   3rd Qu.: 729.0  \n",
       " Max.   :2471.0   Max.   :28974   Max.   :14957   Max.   :6794.0  \n",
       "     master             doc              nHouse          latitude    \n",
       " Min.   :    0.0   Min.   :   0.00   Min.   :   0.0   Min.   :25.56  \n",
       " 1st Qu.:   46.0   1st Qu.:   0.00   1st Qu.: 150.0   1st Qu.:39.19  \n",
       " Median :  129.0   Median :   8.00   Median : 200.0   Median :40.41  \n",
       " Mean   :  350.4   Mean   :  48.16   Mean   : 346.4   Mean   :40.25  \n",
       " 3rd Qu.:  370.0   3rd Qu.:  36.00   3rd Qu.: 450.0   3rd Qu.:41.32  \n",
       " Max.   :10756.0   Max.   :3632.00   Max.   :2600.0   Max.   :47.44  \n",
       "   longitude            pop            pnMale          pnFemale     \n",
       " Min.   :-123.17   Min.   :    0   Min.   :0.3212   Min.   :0.1050  \n",
       " 1st Qu.: -90.05   1st Qu.: 1761   1st Qu.:0.4861   1st Qu.:0.4873  \n",
       " Median : -87.48   Median : 4297   Median :0.4990   Median :0.5010  \n",
       " Mean   : -88.05   Mean   : 8554   Mean   :0.5014   Mean   :0.4986  \n",
       " 3rd Qu.: -85.00   3rd Qu.:10769   3rd Qu.:0.5127   3rd Qu.:0.5139  \n",
       " Max.   : -71.10   Max.   :80489   Max.   :0.8950   Max.   :0.6788  \n",
       "     pwhite           pblack              pnatAm              pasian        \n",
       " Min.   :0.1378   Min.   :0.0000000   Min.   :0.0000000   Min.   :0.000000  \n",
       " 1st Qu.:0.9342   1st Qu.:0.0006882   1st Qu.:0.0000000   1st Qu.:0.000000  \n",
       " Median :0.9658   Median :0.0044999   Median :0.0004654   Median :0.001899  \n",
       " Mean   :0.9453   Mean   :0.0207881   Mean   :0.0031864   Mean   :0.008208  \n",
       " 3rd Qu.:0.9836   3rd Qu.:0.0170411   3rd Qu.:0.0029018   3rd Qu.:0.007905  \n",
       " Max.   :1.0000   Max.   :0.8233336   Max.   :0.5596346   Max.   :0.770408  \n",
       "     phisp            pcAlone           pcPool            pcPub          \n",
       " Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000000  \n",
       " 1st Qu.:0.01006   1st Qu.:0.3550   1st Qu.:0.02816   1st Qu.:0.0000000  \n",
       " Median :0.02350   Median :0.3908   Median :0.03541   Median :0.0002374  \n",
       " Mean   :0.03351   Mean   :0.3893   Mean   :0.03825   Mean   :0.0015819  \n",
       " 3rd Qu.:0.03791   3rd Qu.:0.4306   3rd Qu.:0.04759   3rd Qu.:0.0018618  \n",
       " Max.   :0.77041   Max.   :0.6986   Max.   :0.32653   Max.   :0.3705416  \n",
       "     pmBach             pfBach            ptotBach          phSchool     \n",
       " Min.   :0.000000   Min.   :0.000000   Min.   :0.00000   Min.   :0.0000  \n",
       " 1st Qu.:0.003841   1st Qu.:0.006319   1st Qu.:0.08862   1st Qu.:0.2031  \n",
       " Median :0.007680   Median :0.009379   Median :0.11681   Median :0.2391  \n",
       " Mean   :0.008408   Mean   :0.010876   Mean   :0.12647   Mean   :0.2369  \n",
       " 3rd Qu.:0.011292   3rd Qu.:0.014383   3rd Qu.:0.14934   3rd Qu.:0.2747  \n",
       " Max.   :0.167798   Max.   :0.115903   Max.   :0.76057   Max.   :1.0000  \n",
       "   pnoSchool          pmaster             pdoc              pnMort      \n",
       " Min.   :0.00000   Min.   :0.00000   Min.   :0.000000   Min.   : 0.000  \n",
       " 1st Qu.:0.04764   1st Qu.:0.02265   1st Qu.:0.000000   1st Qu.: 1.907  \n",
       " Median :0.06296   Median :0.03002   Median :0.001898   Median : 3.270  \n",
       " Mean   :0.07017   Mean   :0.03318   Mean   :0.003422   Mean   : 3.429  \n",
       " 3rd Qu.:0.08767   3rd Qu.:0.03969   3rd Qu.:0.004497   3rd Qu.: 4.485  \n",
       " Max.   :1.00000   Max.   :0.24554   Max.   :0.085885   Max.   :22.240  \n",
       "    pnNoMort        pmortStat     \n",
       " Min.   : 0.000   Min.   : 0.000  \n",
       " 1st Qu.: 1.620   1st Qu.: 3.587  \n",
       " Median : 2.456   Median : 5.958  \n",
       " Mean   : 2.510   Mean   : 5.939  \n",
       " 3rd Qu.: 3.095   3rd Qu.: 7.622  \n",
       " Max.   :19.159   Max.   :30.441  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#split train into train/test subsets\n",
    "\n",
    "datum_train = sample( nrow(train) , as.integer(nrow(train)*.8) )\n",
    "train_1 = train[datum_train,]\n",
    "train_2 = train[-datum_train,]\n",
    "\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(predvars, data, env): object 'PCA1' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(predvars, data, env): object 'PCA1' not found\nTraceback:\n",
      "1. gbm(PastDue ~ CreditScore + TYCornUnits + TYBeanUnits + TYWheatUnits + \n .     LYCornUnits + LYBeanUnits + LYWheatUnits + houseIncome + \n .     age + singIncome + nMale + nFemale + medAge + medAgeMale + \n .     medAgeFemale + cAlone + cPool + cPub + nMort + nNoMort + \n .     mortStat + mBach + fBach + totBach + hSchool + noSchool + \n .     master + doc + G094 + MT36 + RE34 + latitude + longitude + \n .     pnMale + pnFemale + pcAlone + pcPool + pcPub + pnMort + pmaster + \n .     pdoc + pnNoMort + pmortStat + pmBach + pfBach + ptotBach + \n .     phSchool + pnoSchool + white + black + asian + natAm + hisp + \n .     pwhite + pblack + pasian + pnatAm + phisp + PCA1 + PCA2 + \n .     PCA3 + PCA4 + PCA5 + PCA6 + PCA7 + PCA8 + PCA9 + PCA0, data = train_1, \n .     n.trees = 1500, interaction.depth = 7)",
      "2. eval(mf, parent.frame())",
      "3. eval(mf, parent.frame())",
      "4. model.frame(formula = PastDue ~ CreditScore + TYCornUnits + TYBeanUnits + \n .     TYWheatUnits + LYCornUnits + LYBeanUnits + LYWheatUnits + \n .     houseIncome + age + singIncome + nMale + nFemale + medAge + \n .     medAgeMale + medAgeFemale + cAlone + cPool + cPub + nMort + \n .     nNoMort + mortStat + mBach + fBach + totBach + hSchool + \n .     noSchool + master + doc + G094 + MT36 + RE34 + latitude + \n .     longitude + pnMale + pnFemale + pcAlone + pcPool + pcPub + \n .     pnMort + pmaster + pdoc + pnNoMort + pmortStat + pmBach + \n .     pfBach + ptotBach + phSchool + pnoSchool + white + black + \n .     asian + natAm + hisp + pwhite + pblack + pasian + pnatAm + \n .     phisp + PCA1 + PCA2 + PCA3 + PCA4 + PCA5 + PCA6 + PCA7 + \n .     PCA8 + PCA9 + PCA0, data = train_1, drop.unused.levels = TRUE, \n .     na.action = function (object, ...) \n .     object)",
      "5. model.frame.default(formula = PastDue ~ CreditScore + TYCornUnits + \n .     TYBeanUnits + TYWheatUnits + LYCornUnits + LYBeanUnits + \n .     LYWheatUnits + houseIncome + age + singIncome + nMale + nFemale + \n .     medAge + medAgeMale + medAgeFemale + cAlone + cPool + cPub + \n .     nMort + nNoMort + mortStat + mBach + fBach + totBach + hSchool + \n .     noSchool + master + doc + G094 + MT36 + RE34 + latitude + \n .     longitude + pnMale + pnFemale + pcAlone + pcPool + pcPub + \n .     pnMort + pmaster + pdoc + pnNoMort + pmortStat + pmBach + \n .     pfBach + ptotBach + phSchool + pnoSchool + white + black + \n .     asian + natAm + hisp + pwhite + pblack + pasian + pnatAm + \n .     phisp + PCA1 + PCA2 + PCA3 + PCA4 + PCA5 + PCA6 + PCA7 + \n .     PCA8 + PCA9 + PCA0, data = train_1, drop.unused.levels = TRUE, \n .     na.action = function (object, ...) \n .     object)",
      "6. eval(predvars, data, env)",
      "7. eval(predvars, data, env)"
     ]
    }
   ],
   "source": [
    "model_gbm_withPer_1 = gbm(PastDue~CreditScore+TYCornUnits+TYBeanUnits+TYWheatUnits+LYCornUnits+\n",
    "                       LYBeanUnits+LYWheatUnits+houseIncome+age+singIncome+nMale+nFemale+\n",
    "                       medAge+medAgeMale+medAgeFemale+cAlone+cPool+cPub+nMort+\n",
    "                       nNoMort+mortStat+mBach+fBach+totBach+hSchool+noSchool+\n",
    "                       master+doc+G094+MT36+RE34+latitude+longitude+pnMale+pnFemale+\n",
    "                       pcAlone+pcPool+pcPub+pnMort+pmaster+pdoc+\n",
    "                       pnNoMort+pmortStat+pmBach+pfBach+ptotBach+phSchool+pnoSchool+\n",
    "                       white+black+asian+natAm+hisp+pwhite+pblack+pasian+pnatAm+phisp+\n",
    "                       PCA1+PCA2+PCA3+PCA4+PCA5+PCA6+PCA7+PCA8+PCA9+PCA0,\n",
    "         data=train_1,n.trees=1500,interaction.depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_glm = glm(PastDue~CreditScore+TYCornUnits+TYBeanUnits+TYWheatUnits+LYCornUnits+\n",
    "#                       LYBeanUnits+LYWheatUnits+houseIncome+age+singIncome+nMale+nFemale+\n",
    "#                       medAge+medAgeMale+medAgeFemale+cAlone+cPool+cPub+nMort+\n",
    "#                       nNoMort+mortStat+mBach+fBach+totBach+hSchool+noSchool+\n",
    "#                       master+doc+G094+MT36+RE34+latitude+longitude+pnMale+pnFemale+\n",
    "#                       pcAlone+pcPool+pcPub+pnMort+pmaster+pdoc+\n",
    "#                       pnNoMort+pmortStat+pmBach+pfBach+ptotBach+phSchool+pnoSchool+\n",
    "#                       white+black+asian+natAm+hisp+pwhite+pblack+pasian+pnatAm+phisp+\n",
    "#                       PCA1+PCA2+PCA3+PCA4+PCA5+PCA6+PCA7+PCA8+PCA9+PCA0,\n",
    "#         data=train_1,family=\"binomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution not specified, assuming bernoulli ...\n"
     ]
    }
   ],
   "source": [
    "model_gbm_withPer_2 = gbm(PastDue~CreditScore+TYCornUnits+TYBeanUnits+TYWheatUnits+LYCornUnits+\n",
    "                       LYBeanUnits+LYWheatUnits+houseIncome+age+singIncome+nMale+nFemale+\n",
    "                       medAge+medAgeMale+medAgeFemale+cAlone+cPool+cPub+nMort+\n",
    "                       nNoMort+mortStat+mBach+fBach+totBach+hSchool+noSchool+\n",
    "                       master+doc+G094+MT36+RE34+latitude+longitude+pnMale+pnFemale+\n",
    "                       pcAlone+pcPool+pcPub+pnMort+pmaster+pdoc+\n",
    "                       pnNoMort+pmortStat+pmBach+pfBach+ptotBach+phSchool+pnoSchool+\n",
    "                       white+black+asian+natAm+hisp+pwhite+pblack+pasian+pnatAm+phisp,\n",
    "         data=train_1,n.trees=1500,interaction.depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution not specified, assuming bernoulli ...\n"
     ]
    }
   ],
   "source": [
    "model_gbm_withPer_3 = gbm(PastDue~CreditScore+TYCornUnits+TYBeanUnits+TYWheatUnits+LYCornUnits+\n",
    "                       LYBeanUnits+LYWheatUnits+houseIncome+age+singIncome+nMale+nFemale+\n",
    "                       medAge+medAgeMale+medAgeFemale+cAlone+cPool+cPub+nMort+\n",
    "                       nNoMort+mortStat+mBach+fBach+totBach+hSchool+noSchool+\n",
    "                       master+doc+G094+MT36+RE34+latitude+longitude+pnMale+pnFemale+\n",
    "                       pcAlone+pcPool+pcPub+pnMort+pmaster+pdoc+\n",
    "                       pnNoMort+pmortStat+pmBach+pfBach+ptotBach+phSchool+pnoSchool+\n",
    "                       white+black+asian+natAm+hisp+pwhite+pblack+pasian+pnatAm+phisp,\n",
    "         data=train_1,n.trees=1500,interaction.depth=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in predict(model_gbm_withPer_1, train_2, n.trees = 1000): object 'model_gbm_withPer_1' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in predict(model_gbm_withPer_1, train_2, n.trees = 1000): object 'model_gbm_withPer_1' not found\nTraceback:\n",
      "1. plogis(predict(model_gbm_withPer_1, train_2, n.trees = 1000))",
      "2. predict(model_gbm_withPer_1, train_2, n.trees = 1000)"
     ]
    }
   ],
   "source": [
    "train_2$predNo = plogis(predict(model_gbm_withPer_1,train_2,n.trees=1000))\n",
    "accuracy(round(train_2$predNo),train_2$PastDue)\n",
    "train_2$predYes = plogis(predict(model_gbm_withPer_2,train_2,n.trees=1000))\n",
    "accuracy(round(train_2$predYes),train_2$PastDue)\n",
    "train_2$predOkay = plogis(predict(model_gbm_withPer_3,train_2,n.trees=1000))\n",
    "accuracy(round(train_2$predOkay),train_2$PastDue)\n",
    "train_2$predNo = plogis(predict(model_gbm_withPer_1,train_2,n.trees=1500))\n",
    "accuracy(round(train_2$predNo),train_2$PastDue)\n",
    "train_2$predYes = plogis(predict(model_gbm_withPer_2,train_2,n.trees=1500))\n",
    "accuracy(round(train_2$predYes),train_2$PastDue)\n",
    "train_2$predOkay = plogis(predict(model_gbm_withPer_3,train_2,n.trees=1500))\n",
    "accuracy(round(train_2$predOkay),train_2$PastDue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_1$\n",
    "#dddd\n",
    "#model_knn = knn(PastDue~CreditScore+TYCornUnits+TYBeanUnits+TYWheatUnits+LYCornUnits+\n",
    "#                       LYBeanUnits+LYWheatUnits+houseIncome+age+singIncome+nMale+nFemale+\n",
    "#                       medAge+medAgeMale+medAgeFemale+cAlone+cPool+cPub+nMort+\n",
    "#                       nNoMort+mortStat+mBach+fBach+totBach+hSchool+noSchool+\n",
    "#                       master+doc+G094+MT36+RE34+latitude+longitude+pnMale+pnFemale+\n",
    "#                       pcAlone+pcPool+pcPub+pnMort+pmaster+pdoc+\n",
    "#                       pnNoMort+pmortStat+pmBach+pfBach+ptotBach+phSchool+pnoSchool+\n",
    "#                       white+black+asian+natAm+hisp+pwhite+pblack+pasian+pnatAm+phisp,\n",
    "#         data=train_1,family=\"binomial\",importance=TRUE)\n",
    "\n",
    "#pred_train_rf = predict(model_rf,train_2,type=\"response\")\n",
    "\n",
    "#accuracy(train_2$PastDue,pred_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in predict(model_gbm_withPer_1, test, n.trees = 1000): object 'model_gbm_withPer_1' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in predict(model_gbm_withPer_1, test, n.trees = 1000): object 'model_gbm_withPer_1' not found\nTraceback:\n",
      "1. plogis(predict(model_gbm_withPer_1, test, n.trees = 1000))",
      "2. predict(model_gbm_withPer_1, test, n.trees = 1000)"
     ]
    }
   ],
   "source": [
    "#Binomail and quasibinomial are the only viable options\n",
    "#we will use binomial and output a submission here\n",
    "test$PastDue = plogis(predict(model_gbm_withPer_1,test,n.trees=1000))\n",
    "write.csv(test[,c(\"id\",\"PastDue\")],\"sub_1.csv\",row.names=F)\n",
    "test$PastDue <- NULL\n",
    "test$PastDue = plogis(predict(model_gbm_withPer_2,test,n.trees=1000))\n",
    "write.csv(test[,c(\"id\",\"PastDue\")],\"sub_2.csv\",row.names=F)\n",
    "test$PastDue <- NULL\n",
    "test$PastDue = plogis(predict(model_gbm_withPer_3,test,n.trees=1000))\n",
    "write.csv(test[,c(\"id\",\"PastDue\")],\"sub_3\",row.names=F)\n",
    "test$PastDue = predict(model_rf,test,type=\"response\")\n",
    "test$PastDue = plogis(predict(model_gbm_withPer_1,test,n.trees=1500))\n",
    "write.csv(test[,c(\"id\",\"PastDue\")],\"sub_11.csv\",row.names=F)\n",
    "test$PastDue <- NULL\n",
    "test$PastDue = plogis(predict(model_gbm_withPer_2,test,n.trees=1500))\n",
    "write.csv(test[,c(\"id\",\"PastDue\")],\"sub_21.csv\",row.names=F)\n",
    "test$PastDue <- NULL\n",
    "test$PastDue = plogis(predict(model_gbm_withPer_3,test,n.trees=1500))\n",
    "write.csv(test[,c(\"id\",\"PastDue\")],\"sub_31\",row.names=F)\n",
    "test$PastDue = predict(model_rf,test,type=\"response\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
