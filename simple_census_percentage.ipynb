{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.2.1.\u001b[31m9000\u001b[39m     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.3     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 2.1.3          \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 0.8.3     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.0.0          \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1          \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.4.0     \n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Loaded gbm 2.1.5\n",
      "\n",
      "randomForest 4.6-14\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘mltools’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:tidyr’:\n",
      "\n",
      "    replace_na\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘data.table’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    between, first, last\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    transpose\n",
      "\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: ‘caret’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:SDMTools’:\n",
      "\n",
      "    sensitivity, specificity\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    lift\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:httr’:\n",
      "\n",
      "    progress\n",
      "\n",
      "\n",
      "Installing package into ‘/usr/local/lib/R/site-library’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'agriculture-loan-prediction'"
      ],
      "text/latex": [
       "'agriculture-loan-prediction'"
      ],
      "text/markdown": [
       "'agriculture-loan-prediction'"
      ],
      "text/plain": [
       "[1] \"agriculture-loan-prediction\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Importing packages\n",
    "\n",
    "# This R environment comes with all of CRAN and many other helpful packages preinstalled.\n",
    "# You can see which packages are installed by checking out the kaggle/rstats docker image: \n",
    "# https://github.com/kaggle/docker-rstats\n",
    "\n",
    "library(tidyverse) # metapackage with lots of helpful functions\n",
    "library(SDMTools)\n",
    "library(gbm)\n",
    "library(randomForest)\n",
    "library(zipcode)\n",
    "library(mltools)\n",
    "library(data.table)\n",
    "library(caret)\n",
    "\n",
    "#devtools::install_github(\"brews/bayfoxr\", lib = \"/kaggle/working\")\n",
    "install.packages(\"tidycensus\")\n",
    "library(tidycensus)\n",
    "\n",
    "## Running code\n",
    "\n",
    "# In a notebook, you can run a single code cell by clicking in the cell and then hitting \n",
    "# the blue arrow to the left, or by clicking in the cell and pressing Shift+Enter. In a script, \n",
    "# you can run code by highlighting the code you want to run and then clicking the blue arrow\n",
    "# at the bottom of this window.\n",
    "\n",
    "## Reading in files\n",
    "\n",
    "# You can access files from datasets you've added to this kernel in the \"../input/\" directory.\n",
    "# You can see the files added to this kernel by running the code below. \n",
    "\n",
    "list.files(path = \"../input\")\n",
    "\n",
    "## Saving data\n",
    "\n",
    "# If you save any files or images, these will be put in the \"output\" directory. You \n",
    "# can see the output directory by committing and running your kernel (using the \n",
    "# Commit & Run button) and then checking out the compiled version of your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in files\n",
    "train=read.csv(\"../input/agriculture-loan-prediction/train_comp1_2020.csv\",stringsAsFactors=FALSE)\n",
    "test=read.csv(\"../input/agriculture-loan-prediction/test_comp1_2020.csv\",stringsAsFactors=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n"
     ]
    }
   ],
   "source": [
    "getmode <- function(v) {\n",
    "   uniqv <- unique(v)\n",
    "   uniqv[which.max(tabulate(match(v, uniqv)))]\n",
    "}\n",
    "\n",
    "#Remove the nulls\n",
    "#head(train$TYCornUnits[!is.numeric(train$TYCornUnits)])\n",
    "\n",
    "train$TYCornUnits=abs(as.numeric(train$TYCornUnits))\n",
    "train$TYCornUnits[is.na(train$TYCornUnits)]=median(train$TYCornUnits[!is.na(train$TYCornUnits)])\n",
    "train$TYBeanUnits=as.numeric(train$TYBeanUnits)\n",
    "train$TYBeanUnits[is.na(train$TYBeanUnits)]=median(train$TYBeanUnits[!is.na(train$TYBeanUnits)])\n",
    "train$TYWheatUnits=as.numeric(train$TYWheatUnits)\n",
    "train$TYWheatUnits[is.na(train$TYWheatUnits)]=median(train$TYWheatUnits[!is.na(train$TYWheatUnits)])\n",
    "train$LYCornUnits=as.numeric(train$LYCornUnits)\n",
    "train$LYCornUnits[is.na(train$LYCornUnits)]=median(train$LYCornUnits[!is.na(train$LYCornUnits)])\n",
    "train$LYBeanUnits=as.numeric(train$LYBeanUnits)\n",
    "train$LYBeanUnits[is.na(train$LYBeanUnits)]=median(train$LYBeanUnits[!is.na(train$LYBeanUnits)])\n",
    "train$LYWheatUnits=as.numeric(train$LYWheatUnits)\n",
    "train$LYWheatUnits[is.na(train$LYWheatUnits)]=median(train$LYWheatUnits[!is.na(train$LYWheatUnits)])\n",
    "train$AT28=as.numeric(train$AT28)\n",
    "train$AT28[is.na(train$AT28)]=median(train$AT28[!is.na(train$AT28)])\n",
    "train$AT33=as.numeric(train$AT33)\n",
    "train$AT33[is.na(train$AT33)]=median(train$AT33[!is.na(train$AT33)])\n",
    "train$AT36=as.numeric(train$AT36)\n",
    "train$AT36[is.na(train$AT36)]=median(train$AT36[!is.na(train$AT36)])\n",
    "train$BC33=as.numeric(train$BC33)\n",
    "train$BC33[is.na(train$BC33)]=median(train$AT36[!is.na(train$BC33)])\n",
    "train$BC98=as.numeric(train$BC98)\n",
    "train$BC98[is.na(train$BC98)]=median(train$BC98[!is.na(train$BC98)])\n",
    "train$G068=as.numeric(train$G068)\n",
    "train$G068[is.na(train$G068)]=median(train$G068[!is.na(train$G068)])\n",
    "train$G091=as.numeric(train$G091)\n",
    "train$G091[is.na(train$G091)]=median(train$G091[!is.na(train$G091)])\n",
    "train$G093=as.numeric(train$G093)\n",
    "train$G093[is.na(train$G093)]=median(train$G093[!is.na(train$G093)])\n",
    "train$G094=as.numeric(train$G094)\n",
    "train$G094[is.na(train$G094)]=median(train$G094[!is.na(train$G094)])\n",
    "train$G096=as.numeric(train$G096)\n",
    "train$G096[is.na(train$G096)]=median(train$G096[!is.na(train$G096)])\n",
    "train$MT28=as.numeric(train$MT28)\n",
    "train$MT28[is.na(train$MT28)]=median(train$MT28[!is.na(train$MT28)])\n",
    "train$MT36=as.numeric(train$MT36)\n",
    "train$MT36[is.na(train$MT36)]=median(train$MT36[!is.na(train$MT36)])\n",
    "train$RE34=as.numeric(train$RE34)\n",
    "train$RE34[is.na(train$RE34)]=median(train$RE34[!is.na(train$RE34)])\n",
    "train$S063=as.numeric(train$S063)\n",
    "train$S063[is.na(train$S063)]=median(train$S063[!is.na(train$S063)])\n",
    "train$Zip_Code=as.numeric(train$Zip_Code)\n",
    "train$Zip_Code[is.na(train$Zip_Code)]=getmode(train$Zip_Code[!is.na(train$Zip_Code)])\n",
    "\n",
    "#train$TYCornUnits=log(abs(train$TYCornUnits+1))\n",
    "#train$TYBeanUnits=log(abs(train$TYBeanUnits+1))\n",
    "#train$TYWheatUnits=log(abs(train$TYWheatUnits+1))\n",
    "#train$LYCornUnits=log(abs(train$LYCornUnits+1))\n",
    "#train$LYBeanUnits=log(abs(train$LYBeanUnits+1))\n",
    "#train$LYWheatUnits=log(abs(train$LYWheatUnits+1))\n",
    "#train$AT28=log(abs(train$AT28+1))\n",
    "#train$AT33=log(abs(train$AT33+1))\n",
    "#train$AT36=log(abs(train$AT36+1))\n",
    "#train$BC33=log(abs(train$BC33+1))\n",
    "#train$BC98=log(abs(train$BC98+1))\n",
    "#train$MT28=log(abs(train$MT28+1))\n",
    "#train$MT36=log(abs(train$MT36+1))\n",
    "#train$RE34=log(abs(train$RE34+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n",
      "Warning message in eval(expr, envir, enclos):\n",
      "“NAs introduced by coercion”\n"
     ]
    }
   ],
   "source": [
    "test$TYCornUnits=abs(as.numeric(test$TYCornUnits))\n",
    "test$TYCornUnits[is.na(test$TYCornUnits)]=median(train$TYCornUnits)\n",
    "test$TYBeanUnits=as.numeric(test$TYBeanUnits)\n",
    "test$TYBeanUnits[is.na(test$TYBeanUnits)]=median(train$TYBeanUnits)\n",
    "test$TYWheatUnits=as.numeric(test$TYWheatUnits)\n",
    "test$TYWheatUnits[is.na(test$TYWheatUnits)]=median(train$TYWheatUnits)\n",
    "test$LYCornUnits=as.numeric(test$LYCornUnits)\n",
    "test$LYCornUnits[is.na(test$LYCornUnits)]=median(train$LYCornUnits)\n",
    "test$LYBeanUnits=as.numeric(test$LYBeanUnits)\n",
    "test$LYBeanUnits[is.na(test$LYBeanUnits)]=median(train$LYBeanUnits)\n",
    "test$LYWheatUnits=as.numeric(test$LYWheatUnits)\n",
    "test$LYWheatUnits[is.na(test$LYWheatUnits)]=median(train$LYWheatUnits)\n",
    "test$AT28=as.numeric(test$AT28)\n",
    "test$AT28[is.na(test$AT28)]=median(train$AT28)\n",
    "test$AT33=as.numeric(test$AT33)\n",
    "test$AT33[is.na(test$AT33)]=median(train$AT33)\n",
    "test$AT36=as.numeric(test$AT36)\n",
    "test$AT36[is.na(test$AT36)]=median(train$AT36)\n",
    "test$BC33=as.numeric(test$BC33)\n",
    "test$BC33[is.na(test$BC33)]=median(train$BC33)\n",
    "test$BC98=as.numeric(test$BC98)\n",
    "test$BC98[is.na(test$BC98)]=median(train$BC98)\n",
    "test$G068=as.numeric(test$G068)\n",
    "test$G068[is.na(test$G068)]=median(train$G068)\n",
    "test$G091=as.numeric(test$G091)\n",
    "test$G091[is.na(test$G091)]=median(train$G091)\n",
    "test$G093=as.numeric(test$G093)\n",
    "test$G093[is.na(test$G093)]=median(train$G093)\n",
    "test$G094=as.numeric(test$G094)\n",
    "test$G094[is.na(test$G094)]=median(train$G094)\n",
    "test$G096=as.numeric(test$G096)\n",
    "test$G096[is.na(test$G096)]=median(train$G096)\n",
    "test$MT28=as.numeric(test$MT28)\n",
    "test$MT28[is.na(test$MT28)]=median(train$MT28)\n",
    "test$MT36=as.numeric(test$MT36)\n",
    "test$MT36[is.na(test$MT36)]=median(train$MT36)\n",
    "test$RE34=as.numeric(test$RE34)\n",
    "test$RE34[is.na(test$RE34)]=median(train$RE34)\n",
    "test$S063=as.numeric(test$S063)\n",
    "test$S063[is.na(test$S063)]=median(train$S063)\n",
    "\n",
    "\n",
    "#test$TYCornUnits=log(abs(test$TYCornUnits+.01))\n",
    "#test$TYBeanUnits=log(abs(test$TYBeanUnits+.01))\n",
    "#test$TYWheatUnits=log(abs(test$TYWheatUnits+.01))\n",
    "#test$LYCornUnits=log(abs(test$LYCornUnits+.01))\n",
    "#test$LYBeanUnits=log(abs(test$LYBeanUnits+.01))\n",
    "#test$LYWheatUnits=log(abs(test$LYWheatUnits+.01))\n",
    "#test$AT28=log(abs(test$AT28+.01))\n",
    "#test$AT33=log(abs(test$AT33+.01))\n",
    "#test$AT36=log(abs(test$AT36+.01))\n",
    "#test$BC33=log(abs(test$BC33+.01))\n",
    "#test$BC98=log(abs(test$BC98+.01))\n",
    "#test$MT28=log(abs(test$MT28+.01))\n",
    "#test$MT36=log(abs(test$MT36+.01))\n",
    "#test$RE34=log(abs(test$RE34+.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To install your API key for use in future sessions, run this function with `install = TRUE`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Census Wrangling\n",
    "\n",
    "census_api_key(\"0e29c36736630228c3e7f39191458ccad51de0e1\")\n",
    "v17 <- load_variables(2017, \"acs5\", cache = TRUE)\n",
    "#View(v17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if(FALSE){\n",
    "    \n",
    "# median household income\n",
    "z <- get_acs(geography = \"zcta\", variables = c(houseIncome=\"B19013_001\")\n",
    "z$Zip_Code = z$GEOCODE\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$estimate[is.na(train$estimate)]=median(train$estimate[!is.na(train$estimate)])\n",
    "train$houseIncome = train$estimate\n",
    "\n",
    "#Age\n",
    "z_2 <- get_acs(geography = \"zcta\", variables = \"B01002_001E\")\n",
    "#Income\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B06011_001\")\n",
    "#num males\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01001_002\")\n",
    "head(z_3)\n",
    "#num females\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01001_026\")\n",
    "head(z_3)\n",
    "#median age\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01002_001\")\n",
    "head(z_3)\n",
    "#median age male\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01002_002\")\n",
    "head(z_3)\n",
    "#median age female\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B01002_003\")\n",
    "head(z_3)\n",
    "\n",
    "             \n",
    "             \n",
    "#Car drove alone\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B08101_009\")\n",
    "head(z_3)\n",
    "#Carpooled\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B08101_017\")\n",
    "head(z_3)\n",
    "#Public Transportation\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B08006_008\")\n",
    "head(z_3)\n",
    "#Num house with mortgages\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B25027_002\")\n",
    "head(z_3)\n",
    "#Num house without mortgages\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B25027_010\")\n",
    "head(z_3)\n",
    "#Mortgage status\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B25081_001\")\n",
    "head(z_3)\n",
    "             \n",
    "#male 25-34 bachelors degree\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15001_017\")\n",
    "head(z_3)\n",
    "#female 25-34 bachelors degree\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15001_058\")\n",
    "head(z_3)\n",
    "#total bachelor degrees\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15011_001\")\n",
    "head(z_3)\n",
    "#high school diploma\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15003_017\")\n",
    "head(z_3)\n",
    "#less than high school\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B16010_002\")\n",
    "head(z_3)\n",
    "#total masters\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15003_023\")\n",
    "head(z_3)\n",
    "#total doctorate\n",
    "z_3 <- get_acs(geography = \"zcta\", variables = \"B15003_025\")\n",
    "head(z_3)\n",
    "             \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#HouseHold Income\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B19013_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,houseIncome = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "hincome = z\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$houseIncome[is.na(train$houseIncome)]=median(train$houseIncome[!is.na(train$houseIncome)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$houseIncome[is.na(test$houseIncome)]=median(test$houseIncome[!is.na(test$houseIncome)])\n",
    "\n",
    "#Age\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01002_001E\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,age = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$age[is.na(train$age)]=median(train$age[!is.na(train$age)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$age[is.na(test$age)]=median(test$age[!is.na(test$age)])\n",
    "\n",
    "#Income\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B06011_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,singIncome = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$singIncome[is.na(train$singIncome)]=median(train$singIncome[!is.na(train$singIncome)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$singIncome[is.na(test$singIncome)]=median(test$singIncome[!is.na(test$singIncome)])\n",
    "\n",
    "#num males\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01001_002\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nMale = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nMale[is.na(train$nMale)]=median(train$nMale[!is.na(train$nMale)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nMale[is.na(test$nMale)]=median(test$nMale[!is.na(test$nMale)])\n",
    "\n",
    "#num femaels\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01001_026\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nFemale = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nFemale[is.na(train$nFemale)]=median(train$nFemale[!is.na(train$nFemale)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nFemale[is.na(test$nFemale)]=median(test$nFemale[!is.na(test$nFemale)])\n",
    "\n",
    "#median age\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01002_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,medAge = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "age = z\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$medAge[is.na(train$medAge)]=median(train$medAge[!is.na(train$medAge)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$medAge[is.na(test$medAge)]=median(test$medAge[!is.na(test$medAge)])\n",
    "\n",
    "#med Age male\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01002_002\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,medAgeMale = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$medAgeMale[is.na(train$medAgeMale)]=median(train$medAgeMale[!is.na(train$medAgeMale)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$medAgeMale[is.na(test$medAgeMale)]=median(test$medAgeMale[!is.na(test$medAgeMale)])\n",
    "\n",
    "#median age femal\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B01002_003\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,medAgeFemale = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$medAgeFemale[is.na(train$medAgeFemale)]=median(train$medAgeFemale[!is.na(train$medAgeFemale)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$medAgeFemale[is.na(test$medAgeFemale)]=median(test$medAgeFemale[!is.na(test$medAgeFemale)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Car drove alone\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B08101_009\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,cAlone = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$cAlone[is.na(train$cAlone)]=median(train$cAlone[!is.na(train$cAlone)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$cAlone[is.na(test$cAlone)]=median(test$cAlone[!is.na(test$cAlone)])\n",
    "\n",
    "#Carpooled\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B08101_017\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,cPool = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$cPool[is.na(train$cPool)]=median(train$cPool[!is.na(train$cPool)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$cPool[is.na(test$cPool)]=median(test$cPool[!is.na(test$cPool)])\n",
    "\n",
    "#public transportation\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B08006_008\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,cPub = estimate)\n",
    "pub = z\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$cPub[is.na(train$cPub)]=median(train$cPub[!is.na(train$cPub)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$cPub[is.na(test$cPub)]=median(test$cPub[!is.na(test$cPub)])\n",
    "\n",
    "\n",
    "#num Mortgages\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B25027_002\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nMort = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "mort = z\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nMort[is.na(train$nMort)]=median(train$nMort[!is.na(train$nMort)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nMort[is.na(test$nMort)]=median(test$nMort[!is.na(test$nMort)])\n",
    "\n",
    "#num no Mortgages\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B25027_010\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nNoMort = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nNoMort[is.na(train$nNoMort)]=median(train$nNoMort[!is.na(train$nNoMort)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nNoMort[is.na(test$nNoMort)]=median(test$nNoMort[!is.na(test$nNoMort)])\n",
    "\n",
    "#Mortgage Status\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B25081_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,mortStat = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$mortStat[is.na(train$mortStat)]=median(train$mortStat[!is.na(train$mortStat)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$mortStat[is.na(test$mortStat)]=median(test$mortStat[!is.na(test$mortStat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n",
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#male 25-34 bachelors degree\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15001_017\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,mBach = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$mBach[is.na(train$mBach)]=median(train$mBach[!is.na(train$mBach)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$mBach[is.na(test$mBach)]=median(test$mBach[!is.na(test$mBach)])\n",
    "\n",
    "#female 25-34 bachelors degree\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15001_058\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,fBach = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$fBach[is.na(train$fBach)]=median(train$fBach[!is.na(train$fBach)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$fBach[is.na(test$fBach)]=median(test$fBach[!is.na(test$fBach)])\n",
    "\n",
    "#total Bachelors degrees\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15011_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,totBach = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$totBach[is.na(train$totBach)]=median(train$totBach[!is.na(train$totBach)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$totBach[is.na(test$totBach)]=median(test$totBach[!is.na(test$totBach)])\n",
    "\n",
    "#high school diploma\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15003_017\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,hSchool = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "skool = z\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$hSchool[is.na(train$hSchool)]=median(train$hSchool[!is.na(train$hSchool)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$hSchool[is.na(test$hSchool)]=median(test$hSchool[!is.na(test$hSchool)])\n",
    "\n",
    "#less than high school\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B16010_002\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,noSchool = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$noSchool[is.na(train$noSchool)]=median(train$noSchool[!is.na(train$noSchool)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$noSchool[is.na(test$noSchool)]=median(test$noSchool[!is.na(test$noSchool)])\n",
    "\n",
    "#total masters\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15003_023\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,master = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$master[is.na(train$master)]=median(train$master[!is.na(train$master)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$master[is.na(test$master)]=median(test$master[!is.na(test$master)])\n",
    "\n",
    "#total doctorate\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B15003_025\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,doc = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$doc[is.na(train$doc)]=median(train$doc[!is.na(train$doc)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$doc[is.na(test$doc)]=median(test$doc[!is.na(test$doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from the 2014-2018 5-year ACS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Zip_Code</th><th scope=col>nHouse</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>601</td><td> 600</td></tr>\n",
       "\t<tr><td>602</td><td>1100</td></tr>\n",
       "\t<tr><td>603</td><td>1700</td></tr>\n",
       "\t<tr><td>606</td><td> 250</td></tr>\n",
       "\t<tr><td>610</td><td> 800</td></tr>\n",
       "\t<tr><td>612</td><td>1900</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Zip\\_Code & nHouse\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 601 &  600\\\\\n",
       "\t 602 & 1100\\\\\n",
       "\t 603 & 1700\\\\\n",
       "\t 606 &  250\\\\\n",
       "\t 610 &  800\\\\\n",
       "\t 612 & 1900\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 2\n",
       "\n",
       "| Zip_Code &lt;dbl&gt; | nHouse &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 601 |  600 |\n",
       "| 602 | 1100 |\n",
       "| 603 | 1700 |\n",
       "| 606 |  250 |\n",
       "| 610 |  800 |\n",
       "| 612 | 1900 |\n",
       "\n"
      ],
      "text/plain": [
       "  Zip_Code nHouse\n",
       "1 601       600  \n",
       "2 602      1100  \n",
       "3 603      1700  \n",
       "4 606       250  \n",
       "5 610       800  \n",
       "6 612      1900  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#total housing units\n",
    "z <- get_acs(geography = \"zcta\", variables = \"B00002_001\")\n",
    "z = rename(z,Zip_Code = GEOID)\n",
    "z$Zip_Code = as.numeric(z$Zip_Code)\n",
    "z = rename(z,nHouse = estimate)\n",
    "z$NAME <- NULL\n",
    "z$variable <- NULL\n",
    "z$moe <- NULL\n",
    "head(z)\n",
    "train = merge(x=train,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "train$nHouse[is.na(train$nHouse)]=median(train$nHouse[!is.na(train$nHouse)])\n",
    "test = merge(x=test,y=z,by = \"Zip_Code\",all.x=TRUE)\n",
    "test$nHouse[is.na(test$nHouse)]=median(test$nHouse[!is.na(test$nHouse)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Zip_Code       CreditScore     TYCornUnits    TYBeanUnits     \n",
       " Min.   :  2139   Min.   :  0.0   Min.   :   0   Min.   :    0.0  \n",
       " 1st Qu.: 45846   1st Qu.:733.0   1st Qu.:  97   1st Qu.:  330.0  \n",
       " Median : 47987   Median :785.0   Median : 160   Median :  495.0  \n",
       " Mean   : 51649   Mean   :766.9   Mean   : 231   Mean   :  688.6  \n",
       " 3rd Qu.: 61272   3rd Qu.:817.0   3rd Qu.: 248   3rd Qu.:  720.0  \n",
       " Max.   :475211   Max.   :850.0   Max.   :5940   Max.   :16335.0  \n",
       "  TYWheatUnits     LYCornUnits      LYBeanUnits       LYWheatUnits   \n",
       " Min.   :   0.0   Min.   :   0.0   Min.   :    0.0   Min.   :   0.0  \n",
       " 1st Qu.: 135.0   1st Qu.:  54.0   1st Qu.:  237.0   1st Qu.: 145.0  \n",
       " Median : 135.0   Median : 108.0   Median :  450.0   Median : 145.0  \n",
       " Mean   : 151.1   Mean   : 181.5   Mean   :  718.4   Mean   : 165.1  \n",
       " 3rd Qu.: 135.0   3rd Qu.: 202.0   3rd Qu.:  810.0   3rd Qu.: 145.0  \n",
       " Max.   :4850.0   Max.   :4616.0   Max.   :18303.0   Max.   :4230.0  \n",
       "    State                AT28               AT33              AT36       \n",
       " Length:99517       Min.   :       0   Min.   :      0   Min.   :  0.00  \n",
       " Class :character   1st Qu.:   41349   1st Qu.:  10829   1st Qu.: 14.00  \n",
       " Mode  :character   Median :  141800   Median :  87999   Median : 14.00  \n",
       "                    Mean   :  373136   Mean   : 276325   Mean   : 15.89  \n",
       "                    3rd Qu.:  380500   3rd Qu.: 277559   3rd Qu.: 14.00  \n",
       "                    Max.   :11441355   Max.   :9999999   Max.   :182.00  \n",
       "      BC33                BC98             COLL                G068         \n",
       " Min.   :      0.0   Min.   : -27224   Length:99517       Min.   :0.00e+00  \n",
       " 1st Qu.:     14.0   1st Qu.:  16462   Class :character   1st Qu.:0.00e+00  \n",
       " Median :     14.0   Median :  16462   Mode  :character   Median :0.00e+00  \n",
       " Mean   :    794.9   Mean   :  17610                      Mean   :5.77e-02  \n",
       " 3rd Qu.:     14.0   3rd Qu.:  16462                      3rd Qu.:0.00e+00  \n",
       " Max.   :1155505.0   Max.   :1462816                      Max.   :2.10e+03  \n",
       "      G091               G093               G094               G096         \n",
       " Min.   :     0.0   Min.   :   0.000   Min.   :0.000000   Min.   :     0.0  \n",
       " 1st Qu.:     0.0   1st Qu.:   0.000   1st Qu.:0.000000   1st Qu.:     1.0  \n",
       " Median :     0.0   Median :   0.000   Median :0.000000   Median :     3.0  \n",
       " Mean   :   225.1   Mean   :   0.113   Mean   :0.004331   Mean   :    19.4  \n",
       " 3rd Qu.:     0.0   3rd Qu.:   0.000   3rd Qu.:0.000000   3rd Qu.:     5.0  \n",
       " Max.   :780330.0   Max.   :4363.000   Max.   :5.000000   Max.   :350209.0  \n",
       "      MT28              MT36            RE34             S063         \n",
       " Min.   :      0   Min.   : 0.00   Min.   :  0.00   Min.   :     0.0  \n",
       " 1st Qu.:      0   1st Qu.:19.00   1st Qu.:  4.70   1st Qu.:     0.0  \n",
       " Median :      0   Median :19.00   Median :  9.60   Median :     0.0  \n",
       " Mean   : 216449   Mean   :19.31   Mean   : 17.45   Mean   :    34.5  \n",
       " 3rd Qu.: 187000   3rd Qu.:19.00   3rd Qu.: 18.80   3rd Qu.:     0.0  \n",
       " Max.   :9999999   Max.   :83.00   Max.   :235.30   Max.   :388430.0  \n",
       "    PastDue              id         houseIncome          age       \n",
       " Min.   :0.00000   Min.   :    1   Min.   : 16354   Min.   :12.90  \n",
       " 1st Qu.:0.00000   1st Qu.:24880   1st Qu.: 48419   1st Qu.:37.80  \n",
       " Median :0.00000   Median :49759   Median : 55417   Median :41.00  \n",
       " Mean   :0.03895   Mean   :49759   Mean   : 56870   Mean   :40.85  \n",
       " 3rd Qu.:0.00000   3rd Qu.:74638   3rd Qu.: 64531   3rd Qu.:44.10  \n",
       " Max.   :1.00000   Max.   :99517   Max.   :216042   Max.   :73.50  \n",
       "   singIncome         nMale          nFemale          medAge     \n",
       " Min.   :  8622   Min.   :    0   Min.   :    0   Min.   :12.90  \n",
       " 1st Qu.: 25853   1st Qu.:  899   1st Qu.:  878   1st Qu.:37.80  \n",
       " Median : 28977   Median : 2203   Median : 2094   Median :41.00  \n",
       " Mean   : 28928   Mean   : 4251   Mean   : 4303   Mean   :40.85  \n",
       " 3rd Qu.: 31220   3rd Qu.: 5378   3rd Qu.: 5506   3rd Qu.:44.10  \n",
       " Max.   :105452   Max.   :40568   Max.   :40724   Max.   :73.50  \n",
       "   medAgeMale     medAgeFemale       cAlone          cPool     \n",
       " Min.   :10.70   Min.   :14.30   Min.   :    0   Min.   :   0  \n",
       " 1st Qu.:36.50   1st Qu.:38.60   1st Qu.:  678   1st Qu.:  61  \n",
       " Median :39.70   Median :42.30   Median : 1829   Median : 127  \n",
       " Mean   :39.73   Mean   :42.01   Mean   : 3304   Mean   : 337  \n",
       " 3rd Qu.:43.00   3rd Qu.:45.50   3rd Qu.: 4060   3rd Qu.: 407  \n",
       " Max.   :72.50   Max.   :73.50   Max.   :36432   Max.   :3853  \n",
       "      cPub             nMort          nNoMort           mortStat    \n",
       " Min.   :   0.00   Min.   :    0   Min.   :    0.0   Min.   :    0  \n",
       " 1st Qu.:   0.00   1st Qu.:  297   1st Qu.:  254.0   1st Qu.:  567  \n",
       " Median :   1.00   Median :  654   Median :  591.0   Median : 1273  \n",
       " Mean   :  22.82   Mean   : 1425   Mean   :  940.9   Mean   : 2366  \n",
       " 3rd Qu.:  11.00   3rd Qu.: 1794   3rd Qu.: 1263.0   3rd Qu.: 3091  \n",
       " Max.   :7250.00   Max.   :16534   Max.   :18201.0   Max.   :28919  \n",
       "     mBach             fBach           totBach         hSchool     \n",
       " Min.   :   0.00   Min.   :   0.0   Min.   :    0   Min.   :    0  \n",
       " 1st Qu.:  10.00   1st Qu.:  16.0   1st Qu.:  199   1st Qu.:  427  \n",
       " Median :  33.00   Median :  35.0   Median :  406   Median : 1101  \n",
       " Mean   :  86.47   Mean   : 107.2   Mean   : 1272   Mean   : 1796  \n",
       " 3rd Qu.:  81.00   3rd Qu.: 112.0   3rd Qu.: 1436   3rd Qu.: 2227  \n",
       " Max.   :2832.00   Max.   :2471.0   Max.   :28974   Max.   :14957  \n",
       "    noSchool          master             doc              nHouse      \n",
       " Min.   :   0.0   Min.   :    0.0   Min.   :   0.00   Min.   :   0.0  \n",
       " 1st Qu.: 104.0   1st Qu.:   46.0   1st Qu.:   0.00   1st Qu.: 150.0  \n",
       " Median : 225.0   Median :  129.0   Median :   8.00   Median : 200.0  \n",
       " Mean   : 606.1   Mean   :  350.4   Mean   :  48.16   Mean   : 346.4  \n",
       " 3rd Qu.: 729.0   3rd Qu.:  370.0   3rd Qu.:  36.00   3rd Qu.: 450.0  \n",
       " Max.   :6794.0   Max.   :10756.0   Max.   :3632.00   Max.   :2600.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>99517</li>\n",
       "\t<li>48</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 99517\n",
       "\\item 48\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 99517\n",
       "2. 48\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 99517    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>99517</li>\n",
       "\t<li>53</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 99517\n",
       "\\item 53\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 99517\n",
       "2. 53\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 99517    53"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(zipcode)\n",
    "#zipcode\n",
    "\n",
    "zipcode$Zip_Code = as.numeric(zipcode$zip)\n",
    "\n",
    "dim(train)\n",
    "train = merge(x=train,y=zipcode,by = \"Zip_Code\",all.x=TRUE)\n",
    "test = merge(x=test,y=zipcode,by = \"Zip_Code\",all.x=TRUE)\n",
    "#Make sure I did that rite\n",
    "dim(train)\n",
    "\n",
    "#We get some null lats + longs - fix it\n",
    "\n",
    "train$latitude[is.na(train$latitude)]=median(train$latitude[!is.na(train$latitude)])\n",
    "train$longitude[is.na(train$longitude)]=median(train$longitude[!is.na(train$longitude)])\n",
    "test$latitude[is.na(test$latitude)]=median(test$latitude[!is.na(test$latitude)])\n",
    "test$longitude[is.na(test$longitude)]=median(test$longitude[!is.na(test$longitude)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the population info\n",
    "train = transform(train, pop = nMale + nFemale)\n",
    "test = transform(test, pop = nMale + nFemale)\n",
    "#train$pop[is.zero(train$pop)]=median(train$pop[!is.zero(train$pop)])\n",
    "#test$pop[is.zero(test$pop)]=median(test$pop[!is.zero(test$pop)])\n",
    "\n",
    "train = transform(train, pnMale = nMale/pop)\n",
    "test = transform(test, pnMale = nMale/pop)\n",
    "train$pnMale[is.na(train$pnMale)]=median(train$pnMale[!is.na(train$pnMale)])\n",
    "test$pnMale[is.na(test$pnMale)]=median(test$pnMale[!is.na(test$pnMale)])\n",
    "train = transform(train, pnFemale = nFemale/pop)\n",
    "test = transform(test, pnFemale = nFemale/pop)\n",
    "train$pnFemale[is.na(train$pnFemale)]=median(train$pnFemale[!is.na(train$pnFemale)])\n",
    "test$pnFemale[is.na(test$pnFemale)]=median(test$pnFemale[!is.na(test$pnFemale)])\n",
    "#Transport\n",
    "train = transform(train, pcAlone = cAlone/pop)\n",
    "test = transform(test, pcAlone = cAlone/pop)\n",
    "train$pcAlone[is.na(train$pcAlone)]=median(train$pcAlone[!is.na(train$pcAlone)])\n",
    "test$pcAlone[is.na(test$pcAlone)]=median(test$pcAlone[!is.na(test$pcAlone)])\n",
    "train = transform(train, pcPool = cPool/pop)\n",
    "test = transform(test, pcPool = cPool/pop)\n",
    "train$pcPool[is.na(train$pcPool)]=median(train$pcPool[!is.na(train$pcPool)])\n",
    "test$pcPool[is.na(test$pcPool)]=median(test$pcPool[!is.na(test$pcPool)])\n",
    "train = transform(train, pcPub = cPub/pop)\n",
    "test = transform(test, pcPub = cPub/pop)\n",
    "train$pcPub[is.na(train$pcPub)]=median(train$pcPub[!is.na(train$pcPub)])\n",
    "test$pcPub[is.na(test$pcPub)]=median(test$pcPub[!is.na(test$pcPub)])\n",
    "#Education\n",
    "train = transform(train, pmBach = mBach/pop)\n",
    "test = transform(test, pmBach = mBach/pop)\n",
    "train$pmBach[is.na(train$pmBach)]=median(train$pmBach[!is.na(train$pmBach)])\n",
    "test$pmBach[is.na(test$pmBach)]=median(test$pmBach[!is.na(test$pmBach)])\n",
    "train = transform(train, pfBach = fBach/pop)\n",
    "test = transform(test, pfBach = fBach/pop)\n",
    "train$pfBach[is.na(train$pfBach)]=median(train$pfBach[!is.na(train$pfBach)])\n",
    "test$pfBach[is.na(test$pfBach)]=median(test$pfBach[!is.na(test$pfBach)])\n",
    "train = transform(train, ptotBach = totBach/pop)\n",
    "test = transform(test, ptotBach = totBach/pop)\n",
    "train$ptotBach[is.na(train$ptotBach)]=median(train$ptotBach[!is.na(train$ptotBach)])\n",
    "test$ptotBach[is.na(test$ptotBach)]=median(test$ptotBach[!is.na(test$ptotBach)])\n",
    "train = transform(train, phSchool = hSchool/pop)\n",
    "test = transform(test, phSchool = hSchool/pop)\n",
    "train$phSchool[is.na(train$phSchool)]=median(train$phSchool[!is.na(train$phSchool)])\n",
    "test$phSchool[is.na(test$phSchool)]=median(test$phSchool[!is.na(test$phSchool)])\n",
    "train = transform(train, pnoSchool = noSchool/pop)\n",
    "test = transform(test, pnoSchool = noSchool/pop)\n",
    "train$pnoSchool[is.na(train$pnoSchool)]=median(train$pnoSchool[!is.na(train$pnoSchool)])\n",
    "test$pnoSchool[is.na(test$pnoSchool)]=median(test$pnoSchool[!is.na(test$pnoSchool)])\n",
    "train = transform(train, pmaster = master/pop)\n",
    "test = transform(test, pmaster = master/pop)\n",
    "train$pmaster[is.na(train$pmaster)]=median(train$pmaster[!is.na(train$pmaster)])\n",
    "test$pmaster[is.na(test$pmaster)]=median(test$pmaster[!is.na(test$pmaster)])\n",
    "train = transform(train, pdoc = doc/pop)\n",
    "test = transform(test, pdoc = doc/pop)\n",
    "train$pdoc[is.na(train$pdoc)]=median(train$pdoc[!is.na(train$pdoc)])\n",
    "test$pdoc[is.na(test$pdoc)]=median(test$pdoc[!is.na(test$pdoc)])\n",
    "\n",
    "\n",
    "#Housing information\n",
    "train = transform(train, pnMort = nMort/nHouse)\n",
    "test = transform(test, pnMort = nMort/nHouse)\n",
    "train$pnMort[is.na(train$pnMort)]=median(train$pnMort[!is.na(train$pnMort)])\n",
    "test$pnMort[is.na(test$pnMort)]=median(test$pnMort[!is.na(test$pnMort)])\n",
    "train = transform(train, pnNoMort = nNoMort/nHouse)\n",
    "test = transform(test, pnNoMort = nNoMort/nHouse)\n",
    "train$pnNoMort[is.na(train$pnNoMort)]=median(train$pnNoMort[!is.na(train$pnNoMort)])\n",
    "test$pnNoMort[is.na(test$pnNoMort)]=median(test$pnNoMort[!is.na(test$pnNoMort)])\n",
    "train = transform(train, pmortStat = mortStat/nHouse)\n",
    "test = transform(test, pmortStat = mortStat/nHouse)\n",
    "train$pmortStat[is.na(train$pmortStat)]=median(train$pmortStat[!is.na(train$pmortStat)])\n",
    "test$pmortStat[is.na(test$pmortStat)]=median(test$pmortStat[!is.na(test$pmortStat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Zip_Code       CreditScore     TYCornUnits    TYBeanUnits     \n",
       " Min.   :  2139   Min.   :  0.0   Min.   :   0   Min.   :    0.0  \n",
       " 1st Qu.: 45846   1st Qu.:733.0   1st Qu.:  97   1st Qu.:  330.0  \n",
       " Median : 47987   Median :785.0   Median : 160   Median :  495.0  \n",
       " Mean   : 51649   Mean   :766.9   Mean   : 231   Mean   :  688.6  \n",
       " 3rd Qu.: 61272   3rd Qu.:817.0   3rd Qu.: 248   3rd Qu.:  720.0  \n",
       " Max.   :475211   Max.   :850.0   Max.   :5940   Max.   :16335.0  \n",
       "  TYWheatUnits     LYCornUnits      LYBeanUnits       LYWheatUnits   \n",
       " Min.   :   0.0   Min.   :   0.0   Min.   :    0.0   Min.   :   0.0  \n",
       " 1st Qu.: 135.0   1st Qu.:  54.0   1st Qu.:  237.0   1st Qu.: 145.0  \n",
       " Median : 135.0   Median : 108.0   Median :  450.0   Median : 145.0  \n",
       " Mean   : 151.1   Mean   : 181.5   Mean   :  718.4   Mean   : 165.1  \n",
       " 3rd Qu.: 135.0   3rd Qu.: 202.0   3rd Qu.:  810.0   3rd Qu.: 145.0  \n",
       " Max.   :4850.0   Max.   :4616.0   Max.   :18303.0   Max.   :4230.0  \n",
       "    State                AT28               AT33              AT36       \n",
       " Length:99517       Min.   :       0   Min.   :      0   Min.   :  0.00  \n",
       " Class :character   1st Qu.:   41349   1st Qu.:  10829   1st Qu.: 14.00  \n",
       " Mode  :character   Median :  141800   Median :  87999   Median : 14.00  \n",
       "                    Mean   :  373136   Mean   : 276325   Mean   : 15.89  \n",
       "                    3rd Qu.:  380500   3rd Qu.: 277559   3rd Qu.: 14.00  \n",
       "                    Max.   :11441355   Max.   :9999999   Max.   :182.00  \n",
       "      BC33                BC98             COLL                G068         \n",
       " Min.   :      0.0   Min.   : -27224   Length:99517       Min.   :0.00e+00  \n",
       " 1st Qu.:     14.0   1st Qu.:  16462   Class :character   1st Qu.:0.00e+00  \n",
       " Median :     14.0   Median :  16462   Mode  :character   Median :0.00e+00  \n",
       " Mean   :    794.9   Mean   :  17610                      Mean   :5.77e-02  \n",
       " 3rd Qu.:     14.0   3rd Qu.:  16462                      3rd Qu.:0.00e+00  \n",
       " Max.   :1155505.0   Max.   :1462816                      Max.   :2.10e+03  \n",
       "      G091               G093               G094               G096         \n",
       " Min.   :     0.0   Min.   :   0.000   Min.   :0.000000   Min.   :     0.0  \n",
       " 1st Qu.:     0.0   1st Qu.:   0.000   1st Qu.:0.000000   1st Qu.:     1.0  \n",
       " Median :     0.0   Median :   0.000   Median :0.000000   Median :     3.0  \n",
       " Mean   :   225.1   Mean   :   0.113   Mean   :0.004331   Mean   :    19.4  \n",
       " 3rd Qu.:     0.0   3rd Qu.:   0.000   3rd Qu.:0.000000   3rd Qu.:     5.0  \n",
       " Max.   :780330.0   Max.   :4363.000   Max.   :5.000000   Max.   :350209.0  \n",
       "      MT28              MT36            RE34             S063         \n",
       " Min.   :      0   Min.   : 0.00   Min.   :  0.00   Min.   :     0.0  \n",
       " 1st Qu.:      0   1st Qu.:19.00   1st Qu.:  4.70   1st Qu.:     0.0  \n",
       " Median :      0   Median :19.00   Median :  9.60   Median :     0.0  \n",
       " Mean   : 216449   Mean   :19.31   Mean   : 17.45   Mean   :    34.5  \n",
       " 3rd Qu.: 187000   3rd Qu.:19.00   3rd Qu.: 18.80   3rd Qu.:     0.0  \n",
       " Max.   :9999999   Max.   :83.00   Max.   :235.30   Max.   :388430.0  \n",
       "    PastDue              id         houseIncome          age       \n",
       " Min.   :0.00000   Min.   :    1   Min.   : 16354   Min.   :12.90  \n",
       " 1st Qu.:0.00000   1st Qu.:24880   1st Qu.: 48419   1st Qu.:37.80  \n",
       " Median :0.00000   Median :49759   Median : 55417   Median :41.00  \n",
       " Mean   :0.03895   Mean   :49759   Mean   : 56870   Mean   :40.85  \n",
       " 3rd Qu.:0.00000   3rd Qu.:74638   3rd Qu.: 64531   3rd Qu.:44.10  \n",
       " Max.   :1.00000   Max.   :99517   Max.   :216042   Max.   :73.50  \n",
       "   singIncome         nMale          nFemale          medAge     \n",
       " Min.   :  8622   Min.   :    0   Min.   :    0   Min.   :12.90  \n",
       " 1st Qu.: 25853   1st Qu.:  899   1st Qu.:  878   1st Qu.:37.80  \n",
       " Median : 28977   Median : 2203   Median : 2094   Median :41.00  \n",
       " Mean   : 28928   Mean   : 4251   Mean   : 4303   Mean   :40.85  \n",
       " 3rd Qu.: 31220   3rd Qu.: 5378   3rd Qu.: 5506   3rd Qu.:44.10  \n",
       " Max.   :105452   Max.   :40568   Max.   :40724   Max.   :73.50  \n",
       "   medAgeMale     medAgeFemale       cAlone          cPool     \n",
       " Min.   :10.70   Min.   :14.30   Min.   :    0   Min.   :   0  \n",
       " 1st Qu.:36.50   1st Qu.:38.60   1st Qu.:  678   1st Qu.:  61  \n",
       " Median :39.70   Median :42.30   Median : 1829   Median : 127  \n",
       " Mean   :39.73   Mean   :42.01   Mean   : 3304   Mean   : 337  \n",
       " 3rd Qu.:43.00   3rd Qu.:45.50   3rd Qu.: 4060   3rd Qu.: 407  \n",
       " Max.   :72.50   Max.   :73.50   Max.   :36432   Max.   :3853  \n",
       "      cPub             nMort          nNoMort           mortStat    \n",
       " Min.   :   0.00   Min.   :    0   Min.   :    0.0   Min.   :    0  \n",
       " 1st Qu.:   0.00   1st Qu.:  297   1st Qu.:  254.0   1st Qu.:  567  \n",
       " Median :   1.00   Median :  654   Median :  591.0   Median : 1273  \n",
       " Mean   :  22.82   Mean   : 1425   Mean   :  940.9   Mean   : 2366  \n",
       " 3rd Qu.:  11.00   3rd Qu.: 1794   3rd Qu.: 1263.0   3rd Qu.: 3091  \n",
       " Max.   :7250.00   Max.   :16534   Max.   :18201.0   Max.   :28919  \n",
       "     mBach             fBach           totBach         hSchool     \n",
       " Min.   :   0.00   Min.   :   0.0   Min.   :    0   Min.   :    0  \n",
       " 1st Qu.:  10.00   1st Qu.:  16.0   1st Qu.:  199   1st Qu.:  427  \n",
       " Median :  33.00   Median :  35.0   Median :  406   Median : 1101  \n",
       " Mean   :  86.47   Mean   : 107.2   Mean   : 1272   Mean   : 1796  \n",
       " 3rd Qu.:  81.00   3rd Qu.: 112.0   3rd Qu.: 1436   3rd Qu.: 2227  \n",
       " Max.   :2832.00   Max.   :2471.0   Max.   :28974   Max.   :14957  \n",
       "    noSchool          master             doc              nHouse      \n",
       " Min.   :   0.0   Min.   :    0.0   Min.   :   0.00   Min.   :   0.0  \n",
       " 1st Qu.: 104.0   1st Qu.:   46.0   1st Qu.:   0.00   1st Qu.: 150.0  \n",
       " Median : 225.0   Median :  129.0   Median :   8.00   Median : 200.0  \n",
       " Mean   : 606.1   Mean   :  350.4   Mean   :  48.16   Mean   : 346.4  \n",
       " 3rd Qu.: 729.0   3rd Qu.:  370.0   3rd Qu.:  36.00   3rd Qu.: 450.0  \n",
       " Max.   :6794.0   Max.   :10756.0   Max.   :3632.00   Max.   :2600.0  \n",
       "     zip                city              state              latitude    \n",
       " Length:99517       Length:99517       Length:99517       Min.   :25.56  \n",
       " Class :character   Class :character   Class :character   1st Qu.:39.19  \n",
       " Mode  :character   Mode  :character   Mode  :character   Median :40.41  \n",
       "                                                          Mean   :40.25  \n",
       "                                                          3rd Qu.:41.32  \n",
       "                                                          Max.   :47.44  \n",
       "   longitude            pop            pnMale          pnFemale     \n",
       " Min.   :-123.17   Min.   :    0   Min.   :0.3212   Min.   :0.1050  \n",
       " 1st Qu.: -90.05   1st Qu.: 1761   1st Qu.:0.4861   1st Qu.:0.4873  \n",
       " Median : -87.48   Median : 4297   Median :0.4990   Median :0.5010  \n",
       " Mean   : -88.05   Mean   : 8554   Mean   :0.5014   Mean   :0.4986  \n",
       " 3rd Qu.: -85.00   3rd Qu.:10769   3rd Qu.:0.5127   3rd Qu.:0.5139  \n",
       " Max.   : -71.10   Max.   :80489   Max.   :0.8950   Max.   :0.6788  \n",
       "    pcAlone           pcPool            pcPub               pmBach        \n",
       " Min.   :0.0000   Min.   :0.00000   Min.   :0.0000000   Min.   :0.000000  \n",
       " 1st Qu.:0.3550   1st Qu.:0.02816   1st Qu.:0.0000000   1st Qu.:0.003841  \n",
       " Median :0.3908   Median :0.03541   Median :0.0002374   Median :0.007680  \n",
       " Mean   :0.3893   Mean   :0.03825   Mean   :0.0015819   Mean   :0.008408  \n",
       " 3rd Qu.:0.4306   3rd Qu.:0.04759   3rd Qu.:0.0018618   3rd Qu.:0.011292  \n",
       " Max.   :0.6986   Max.   :0.32653   Max.   :0.3705416   Max.   :0.167798  \n",
       "     pfBach            ptotBach          phSchool        pnoSchool      \n",
       " Min.   :0.000000   Min.   :0.00000   Min.   :0.0000   Min.   :0.00000  \n",
       " 1st Qu.:0.006319   1st Qu.:0.08862   1st Qu.:0.2031   1st Qu.:0.04764  \n",
       " Median :0.009379   Median :0.11681   Median :0.2391   Median :0.06296  \n",
       " Mean   :0.010876   Mean   :0.12647   Mean   :0.2369   Mean   :0.07017  \n",
       " 3rd Qu.:0.014383   3rd Qu.:0.14934   3rd Qu.:0.2747   3rd Qu.:0.08767  \n",
       " Max.   :0.115903   Max.   :0.76057   Max.   :1.0000   Max.   :1.00000  \n",
       "    pmaster             pdoc              pnMort          pnNoMort     \n",
       " Min.   :0.00000   Min.   :0.000000   Min.   : 0.000   Min.   : 0.000  \n",
       " 1st Qu.:0.02265   1st Qu.:0.000000   1st Qu.: 1.907   1st Qu.: 1.620  \n",
       " Median :0.03002   Median :0.001898   Median : 3.270   Median : 2.456  \n",
       " Mean   :0.03318   Mean   :0.003422   Mean   : 3.429   Mean   : 2.510  \n",
       " 3rd Qu.:0.03969   3rd Qu.:0.004497   3rd Qu.: 4.485   3rd Qu.: 3.095  \n",
       " Max.   :0.24554   Max.   :0.085885   Max.   :22.240   Max.   :19.159  \n",
       "   pmortStat     \n",
       " Min.   : 0.000  \n",
       " 1st Qu.: 3.587  \n",
       " Median : 5.958  \n",
       " Mean   : 5.939  \n",
       " 3rd Qu.: 7.622  \n",
       " Max.   :30.441  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#split train into train/test subsets\n",
    "datum_train = sample( nrow(train) , as.integer(nrow(train)*.8) )\n",
    "train_1 = train[datum_train,]\n",
    "train_2 = train[-datum_train,]\n",
    "\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution not specified, assuming bernoulli ...\n"
     ]
    }
   ],
   "source": [
    "#Regression num loans\n",
    "model_gbm_noPer = gbm(PastDue~CreditScore+TYCornUnits+TYBeanUnits+TYWheatUnits+LYCornUnits+\n",
    "                       LYBeanUnits+LYWheatUnits+houseIncome+age+singIncome+nMale+nFemale+\n",
    "                       medAge+medAgeMale+medAgeFemale+cAlone+cPool+cPub+nMort+\n",
    "                       nNoMort+mortStat+mBach+fBach+totBach+hSchool+noSchool+\n",
    "                       master+doc+G094+MT36+RE34+latitude+longitude,\n",
    "         data=train_1,n.trees=750,interaction.depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution not specified, assuming bernoulli ...\n"
     ]
    }
   ],
   "source": [
    "model_gbm_withPer = gbm(PastDue~CreditScore+TYCornUnits+TYBeanUnits+TYWheatUnits+LYCornUnits+\n",
    "                       LYBeanUnits+LYWheatUnits+houseIncome+age+singIncome+nMale+nFemale+\n",
    "                       medAge+medAgeMale+medAgeFemale+cAlone+cPool+cPub+nMort+\n",
    "                       nNoMort+mortStat+mBach+fBach+totBach+hSchool+noSchool+\n",
    "                       master+doc+G094+MT36+RE34+latitude+longitude+pnMale+pnFemale+\n",
    "                       pcAlone+pcPool+pcPub+pnMort+pmaster+pdoc+\n",
    "                       pnNoMort+pmortStat+pmBach+pfBach+ptotBach+phSchool+pnoSchool,\n",
    "         data=train_1,n.trees=750,interaction.depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution not specified, assuming bernoulli ...\n"
     ]
    }
   ],
   "source": [
    "model_gbm_onlyPer = gbm(PastDue~CreditScore+TYCornUnits+TYBeanUnits+TYWheatUnits+LYCornUnits+\n",
    "                       LYBeanUnits+LYWheatUnits+houseIncome+age+singIncome+\n",
    "                       medAge+medAgeMale+medAgeFemale+\n",
    "                       G094+MT36+RE34+latitude+longitude+pnMale+pnFemale+\n",
    "                       pcAlone+pcPool+pcPub+pnMort+pmaster+pdoc+\n",
    "                       pnNoMort+pmortStat+pmBach+pfBach+ptotBach+phSchool+pnoSchool,\n",
    "         data=train_1,n.trees=750,interaction.depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 1 × 7</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>threshold</th><th scope=col>AUC</th><th scope=col>omission.rate</th><th scope=col>sensitivity</th><th scope=col>specificity</th><th scope=col>prop.correct</th><th scope=col>Kappa</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.5</td><td>0.8299172</td><td>0.3037975</td><td>0.6962025</td><td>0.9636318</td><td>0.9625703</td><td>0.1223317</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 7\n",
       "\\begin{tabular}{lllllll}\n",
       " threshold & AUC & omission.rate & sensitivity & specificity & prop.correct & Kappa\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.5 & 0.8299172 & 0.3037975 & 0.6962025 & 0.9636318 & 0.9625703 & 0.1223317\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 7\n",
       "\n",
       "| threshold &lt;dbl&gt; | AUC &lt;dbl&gt; | omission.rate &lt;dbl&gt; | sensitivity &lt;dbl&gt; | specificity &lt;dbl&gt; | prop.correct &lt;dbl&gt; | Kappa &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 0.5 | 0.8299172 | 0.3037975 | 0.6962025 | 0.9636318 | 0.9625703 | 0.1223317 |\n",
       "\n"
      ],
      "text/plain": [
       "  threshold AUC       omission.rate sensitivity specificity prop.correct\n",
       "1 0.5       0.8299172 0.3037975     0.6962025   0.9636318   0.9625703   \n",
       "  Kappa    \n",
       "1 0.1223317"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 1 × 7</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>threshold</th><th scope=col>AUC</th><th scope=col>omission.rate</th><th scope=col>sensitivity</th><th scope=col>specificity</th><th scope=col>prop.correct</th><th scope=col>Kappa</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.5</td><td>0.8482904</td><td>0.2674419</td><td>0.7325581</td><td>0.9640226</td><td>0.9630225</td><td>0.1394774</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 7\n",
       "\\begin{tabular}{lllllll}\n",
       " threshold & AUC & omission.rate & sensitivity & specificity & prop.correct & Kappa\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.5 & 0.8482904 & 0.2674419 & 0.7325581 & 0.9640226 & 0.9630225 & 0.1394774\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 7\n",
       "\n",
       "| threshold &lt;dbl&gt; | AUC &lt;dbl&gt; | omission.rate &lt;dbl&gt; | sensitivity &lt;dbl&gt; | specificity &lt;dbl&gt; | prop.correct &lt;dbl&gt; | Kappa &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 0.5 | 0.8482904 | 0.2674419 | 0.7325581 | 0.9640226 | 0.9630225 | 0.1394774 |\n",
       "\n"
      ],
      "text/plain": [
       "  threshold AUC       omission.rate sensitivity specificity prop.correct\n",
       "1 0.5       0.8482904 0.2674419     0.7325581   0.9640226   0.9630225   \n",
       "  Kappa    \n",
       "1 0.1394774"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 1 × 7</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>threshold</th><th scope=col>AUC</th><th scope=col>omission.rate</th><th scope=col>sensitivity</th><th scope=col>specificity</th><th scope=col>prop.correct</th><th scope=col>Kappa</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.5</td><td>0.888076</td><td>0.1891892</td><td>0.8108108</td><td>0.9653413</td><td>0.9644795</td><td>0.195077</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 7\n",
       "\\begin{tabular}{lllllll}\n",
       " threshold & AUC & omission.rate & sensitivity & specificity & prop.correct & Kappa\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.5 & 0.888076 & 0.1891892 & 0.8108108 & 0.9653413 & 0.9644795 & 0.195077\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 7\n",
       "\n",
       "| threshold &lt;dbl&gt; | AUC &lt;dbl&gt; | omission.rate &lt;dbl&gt; | sensitivity &lt;dbl&gt; | specificity &lt;dbl&gt; | prop.correct &lt;dbl&gt; | Kappa &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 0.5 | 0.888076 | 0.1891892 | 0.8108108 | 0.9653413 | 0.9644795 | 0.195077 |\n",
       "\n"
      ],
      "text/plain": [
       "  threshold AUC      omission.rate sensitivity specificity prop.correct\n",
       "1 0.5       0.888076 0.1891892     0.8108108   0.9653413   0.9644795   \n",
       "  Kappa   \n",
       "1 0.195077"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_2$predNo = plogis(predict(model_gbm_noPer,train_2,n.trees=300))\n",
    "accuracy(round(train_2$predNo),train_2$PastDue)\n",
    "train_2$predYes = plogis(predict(model_gbm_withPer,train_2,n.trees=300))\n",
    "accuracy(round(train_2$predYes),train_2$PastDue)\n",
    "train_2$predOkay = plogis(predict(model_gbm_onlyPer,train_2,n.trees=500))\n",
    "accuracy(round(train_2$predOkay),train_2$PastDue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binomail and quasibinomial are the only viable options\n",
    "#we will use binomial and output a submission here\n",
    "test$PastDue = plogis(predict(model_gbm_noPer,test,n.trees=300))\n",
    "write.csv(test[,c(\"id\",\"PastDue\")],\"sub_noPer.csv\",row.names=F)\n",
    "test$PastDue <- NULL\n",
    "test$PastDue = plogis(predict(model_gbm_withPer,test,n.trees=300))\n",
    "write.csv(test[,c(\"id\",\"PastDue\")],\"sub_both.csv\",row.names=F)\n",
    "test$PastDue <- NULL\n",
    "test$PastDue = plogis(predict(model_gbm_onlyPer,test,n.trees=500))\n",
    "write.csv(test[,c(\"id\",\"PastDue\")],\"sub_onlyPer.csv\",row.names=F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
